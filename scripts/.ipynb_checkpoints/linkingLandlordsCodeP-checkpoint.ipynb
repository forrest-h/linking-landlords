{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd4aab6-4b32-4523-beef-67340c2ae84e",
   "metadata": {},
   "source": [
    "# Linking Landlords\n",
    "## Linking entities in tax assessment and corporate records\n",
    "#### *Forrest Hangen*\n",
    "Questions/Help: hangen.f@northeastern.edu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "279efeb6-88ca-4ca8-9fac-1091968541af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Packages\n",
    "\n",
    "# pandas, numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# regex, string, word2number, inflect for string cleaning\n",
    "import regex as re\n",
    "import string\n",
    "from word2number import w2n\n",
    "import inflect\n",
    "\n",
    "\n",
    "# math, random\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Levenshtein, itertools, networkx for fuzzy-matching\n",
    "import Levenshtein as lev\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nmslib\n",
    "\n",
    "# multiprocessing for speeding up code (could be modified to be done without this)\n",
    "from multiprocessing import  Pool\n",
    "import multiprocessing\n",
    "\n",
    "# remove warnings (unimport this for troubleshooting)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Parallel processing\n",
    "def parallelize_dataframe(df, func, n_cores=8):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "948bcc51-08ee-4f1a-bba9-4cbeb2257524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Text Cleaning Names and Addresses Functions\n",
    "\n",
    "\n",
    "# Main text cleaning function for names, basics:\n",
    "## Remove punctuation, standardizes spaces\n",
    "## Changes numbers spelled out to numbers (e.g., twelve to 12)\n",
    "## Changes a list of common abbreviations and misspellings (This should be adapted based on the corpus)\n",
    "## Makes all text uppercase\n",
    "## Standardizes cardinal directions\n",
    "## Fix for common misspellings of banks (This should be adapted based on the corpus)\n",
    "\n",
    "\n",
    "\n",
    "# Main text cleaning function\n",
    "# Ex: '81 - EIGHTY-five second RED &  2nd Green.,   St L.L.C' --> '81 - 85 2ND RED AND 2ND GREEN ST LLC'\n",
    "def clean_names(text):\n",
    "    \n",
    "    try:\n",
    "        # Punc/Spaces/Symbols\n",
    "        text = delete_symbols_spaces(text)\n",
    "        text = ' '.join(first_nums(text.split()))\n",
    "        \n",
    "        # Save hyphens for later\n",
    "        text = text.replace('-', ' A%A ')\n",
    "\n",
    "        ## Numbers\n",
    "        text_list = combine_numbers([str(words_to_num(x)) for x in text.split()])\n",
    "        text_list = [convert_mixed(x) for x in text_list]\n",
    "        text_list = [words_to_num(x) for x in ' '.join(text_list).split()]\n",
    "        text_list = combine_numbers([words_to_num(x) for x in text_list])\n",
    "        text_list = [convert_ordinals(x) for x in text_list]\n",
    "\n",
    "        # # Abbreviations\n",
    "        text_list = [convert_abbreviations(str(x).upper(), corp_abb) for x in text_list]   \n",
    "        text = ' '.join([str(x) for x in text_list]).upper()\n",
    "\n",
    "        text = text.replace('A%A', '')\n",
    "        text = delete_symbols_spaces(text)\n",
    "        text = text.replace(' / ', '/')\n",
    "\n",
    "\n",
    "        # # After abbreviations\n",
    "        text =  re.sub(r'(?<=\\b[A-Z]) (?=[A-Z]\\b)', '', text)\n",
    "        text =  re.sub(r'( FR[\\d]/[\\d])', '', text)\n",
    "\n",
    "        # # Misc\n",
    "        # # Switch The to the front\n",
    "        text = switch_the(text)\n",
    "\n",
    "        text = re.sub(r'(0 )(\\d*(?:ST|ND|RD|TH))', '\\g<2>', text)\n",
    "        text = convert_abbreviations_spaces(text, corp_abb_2)\n",
    "        text = convert_nesw(text)\n",
    "        text = fix_banks(text, banks)\n",
    "        \n",
    "        return text\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Remove symbols and extra spaces\n",
    "# Ex: '81 - EIGHTY-five RED & Green.,   Street L.L.C' --> '81 - EIGHTY-FIVE RED AND GREEN STREET L L C'\n",
    "def delete_symbols_spaces(text):\n",
    "    try:\n",
    "        text = text.replace('&', 'and')\n",
    "        #text = text.replace('-', ' ')\n",
    "        text = text.translate(str.maketrans(string.punctuation.replace('/','').replace('-',''), ' '*len(string.punctuation.replace('/','').replace('-',''))))\n",
    "        text = text.replace('/', ' / ')\n",
    "        text = [x.strip() for x in text.split(' ')]\n",
    "        text = [x for x in text if x]\n",
    "        text = ' '.join(text).upper()\n",
    "        return text\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# Check if a number\n",
    "def isnum(text):\n",
    "    try:\n",
    "        t = int(text)\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "# If the first word is a number (before a hyphen) then change the next one to a number as well\n",
    "# Ex: '81 - EIGHTY-FIVE RED AND GREEN STREET L L C' --> \n",
    "def first_nums(tl):\n",
    "    tl2 = []\n",
    "    for x in tl:\n",
    "        if '-' in x:\n",
    "            wt = [isnum(t) for t in x.split('-')]\n",
    "            wn = sum(wt)\n",
    "            if wn == 0:\n",
    "                tl2.append(str(words_to_num(x)))\n",
    "            else:\n",
    "                if len(x.split('-')[1]) == 1:\n",
    "                    p = inflect.engine()\n",
    "                    temp = p.number_to_words(x.split('-')[1])\n",
    "                    tl2.append(str(words_to_num(x.split('-')[0] +'-' +temp)))\n",
    "\n",
    "                else:\n",
    "                    tl2.append(x)\n",
    "\n",
    "\n",
    "        else:\n",
    "            tl2.append(x)\n",
    "        \n",
    "    return tl2\n",
    "    \n",
    "    \n",
    "# Changing any numbers to numerals\n",
    "# Ex: Two --> 2\n",
    "def words_to_num(text):\n",
    "    if text == 'POINT':\n",
    "        return text\n",
    "    else:\n",
    "        try:\n",
    "            return w2n.word_to_num(text)\n",
    "        except:\n",
    "            return text\n",
    "        \n",
    "# Combining numbers \n",
    "# Ex: fifty six --> 50, 6 --> 56\n",
    "def combine_numbers(text_list):\n",
    "    whole_list = []\n",
    "    start = False\n",
    "    end = False\n",
    "    text_list.append('JUNK')\n",
    "    numbers = []\n",
    "    for p in text_list:\n",
    "        try:\n",
    "            int(p)\n",
    "            numbers.append(p)\n",
    "            start = True\n",
    "        except:\n",
    "\n",
    "            if start is True:\n",
    "                end = True\n",
    "            else: \n",
    "                end = False\n",
    "                whole_list.append(p)\n",
    "        if start and end:\n",
    "\n",
    "            if len(numbers) == 1:\n",
    "                whole_list.append(numbers[0])\n",
    "            elif len(numbers) == 2:\n",
    "                if str(numbers[0])[-1:] == '0' and str(numbers[1])[-1:] != '0':\n",
    "                    complete = str(numbers[0] + numbers[1])\n",
    "                else:    \n",
    "                    complete = str(numbers[0]) + str(numbers[1])\n",
    "                whole_list.append(complete)\n",
    "            else:\n",
    "                if str(numbers[0])[-1:] == '0':\n",
    "                    complete = str(numbers[0][:-1] + numbers[1])\n",
    "                else:    \n",
    "                    complete = str(numbers[0]) + str(numbers[1])\n",
    "                for i in numbers[2:]:\n",
    "                    if complete[-1:] == '0':\n",
    "                        complete = str(int(complete) + int(i))\n",
    "                    else:\n",
    "                        complete = complete + str(i)\n",
    "                whole_list.append(complete)\n",
    "            numbers = []\n",
    "            start = False\n",
    "            end = False\n",
    "            whole_list.append(p)\n",
    "    return whole_list[:-1]\n",
    "\n",
    "\n",
    "def convert_ordinals(text):\n",
    "    try:\n",
    "        if (type(words_to_num(text.split('TH')[0])) == int) and (text[-2:] == 'TH'):\n",
    "            text = str(words_to_num(text.split('TH')[0])) + 'TH'\n",
    "        return text\n",
    "    except:\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def convert_mixed(text):\n",
    "    try:\n",
    "        convert = re.sub(r'\\d', '', text)\n",
    "        if type(words_to_num(convert)) == int:\n",
    "            text = re.sub(r'{}'.format(convert), ' ' + str(words_to_num(convert)) + ' ', text)\n",
    "            text = text.strip()\n",
    "        return text\n",
    "    except:\n",
    "        return text\n",
    "    \n",
    "\n",
    "\n",
    "# Common abbreviations\n",
    "# Includes Boston area specifics, should be adapted to corpus \n",
    "\n",
    "def convert_abbreviations(text, corp_abb):\n",
    "    try:\n",
    "        return corp_abb[text]\n",
    "    except:\n",
    "        return text\n",
    "    \n",
    "corp_abb = dict()\n",
    "corp_abb.update(dict.fromkeys([\"REALTY T\", \"RT\"], \"REALTY TRUST\"))\n",
    "corp_abb.update(dict.fromkeys([\"HOSP\"], \"HOSPITAL\"))\n",
    "corp_abb.update(dict.fromkeys([\"FCL\"], \"FORECLOSURE\"))\n",
    "corp_abb.update(dict.fromkeys([\"HOPITAL\"], \"HOSPITAL\"))\n",
    "corp_abb.update(dict.fromkeys([\"L L C\", \"L.L.C.\", \"PLLC\", \"LLLC\", \"LL\"], \"LLC\"))\n",
    "corp_abb.update(dict.fromkeys([\"CTR\", \"CNTR\"], \"CENTER\"))\n",
    "corp_abb.update(dict.fromkeys([\"ASSOC\", \"ASSN\", \"ASSOCIATIO\", \"ASSOCS\", 'ASSOCIATES', 'ASSOCIATE'], \"ASSOCIATION\"))\n",
    "corp_abb.update(dict.fromkeys([\"RLTY\", \"RE\", \"REL\"], \"REALTY\"))\n",
    "corp_abb.update(dict.fromkeys([\"LIMITED PARTNERSHIP\", \"LIMITED PARTNER\", \"LIMITED PARTNERS\", \"LPS\", \"LP\", \"LMTD\", \"LTD\", \"LLP\"], \"LIMITED\"))\n",
    "corp_abb.update(dict.fromkeys([\"SQ\"], \"SQUARE\"))\n",
    "corp_abb.update(dict.fromkeys([\"GROUPS\", \"GRP\", \"GR\"], \"GROUP\"))\n",
    "corp_abb.update(dict.fromkeys([\"APTS\", \"APTMTS\", \"APT\"], \"APARTMENTS\"))\n",
    "corp_abb.update(dict.fromkeys([\"IN\", \"INIT\", \"INTV\"], \"INITIATIVE\"))\n",
    "corp_abb.update(dict.fromkeys([\"TR\", \"TRUSTS\", \"TRUS\", \"TS\", \"TRS\", \"TRSTS\", \"TRST\"], \"TRUST\"))\n",
    "corp_abb.update(dict.fromkeys([\"REVCABLE\"], \"REVOCABLE\"))\n",
    "corp_abb.update(dict.fromkeys([\"PRTNS\", \"PTNRS\", \"PTNR\", \"PRTN\", \"PRTNR\", \"PRTNRS\", \"PTN\", \"PTNS\", \"PARTNER\", \"PARTNERDS\"], \"PARTNERS\"))\n",
    "corp_abb.update(dict.fromkeys([\"PARTNERSIP\"], \"PARTNERSHIP\"))\n",
    "corp_abb.update(dict.fromkeys([\"ETAL\"], \"\"))\n",
    "corp_abb.update(dict.fromkeys([\"AVENUE\"], \"AVE\"))\n",
    "corp_abb.update(dict.fromkeys([\"STREET\", \"STREEET\", \"STRRET\"], \"ST\"))\n",
    "corp_abb.update(dict.fromkeys([\"PK\"], \"PARK\"))\n",
    "corp_abb.update(dict.fromkeys([\"CO-OPERATIVE\", \"CO OPERATIVE\", \"COOP\", \"CO-OP\", \"COOPERA\"], \"COOPERATIVE\"))\n",
    "corp_abb.update(dict.fromkeys([\"INC.\", \"INCOR\", \"INCRP\", \"INCORPORATED\", \"INCORPORTED\"], \"INC\"))\n",
    "corp_abb.update(dict.fromkeys([\"CO\", \"COMANY\", \"COP\"], \"COMPANY\"))\n",
    "corp_abb.update(dict.fromkeys([\"FAM\"], \"FAMILY\"))\n",
    "corp_abb.update(dict.fromkeys([\"INVESMENT\", \"INVESTMENT\", \"INVES\", \"INVESTEMENTS\", \"INVESTMNT\", \"INVESTMNTS\", \"INVEST\", 'INV'], \"INVESTMENTS\"))\n",
    "corp_abb.update(dict.fromkeys([\"TRSTEES\"], \"TRUSTEES\"))\n",
    "corp_abb.update(dict.fromkeys([\"AUTH\"], \"AUTHORITY\"))\n",
    "corp_abb.update(dict.fromkeys([\"BOSTO\", \"BOSTN\"], \"BOSTON\"))\n",
    "corp_abb.update(dict.fromkeys([\"BOSTON HOUSING AUTH\", \"BHA\"], \"BOSTON HOUSING AUTHORITY\"))\n",
    "corp_abb.update(dict.fromkeys([\"HOUSNG\", \"HSNG\"], \"HOUSING\"))\n",
    "corp_abb.update(dict.fromkeys([\"CONDOMINIUM\", \"CONDOMINIUMS\", \"CONDOS\", \"CONDOS\", \"COND\", 'CD'], \"CONDO\"))\n",
    "corp_abb.update(dict.fromkeys([\"CORP\", \"CP\", \"CORPORAITON\", \"CRP\", \"CORPORTATION\", \"CORPORATON\", \"CORPORLATION\"], \"CORPORATION\"))\n",
    "corp_abb.update(dict.fromkeys([\"TRANSP\"], \"TRANSPORTATION\"))\n",
    "corp_abb.update(dict.fromkeys([\"SOC\"], \"SOCIETY\"))\n",
    "corp_abb.update(dict.fromkeys([\"MED\"], \"MEDICAL\"))\n",
    "corp_abb.update(dict.fromkeys([\"NEW ENG\"], \"NEW ENGLAND\"))\n",
    "corp_abb.update(dict.fromkeys([\"SYST\"], \"SYSTEM\"))\n",
    "corp_abb.update(dict.fromkeys([\"SERV\"], \"SERVICES\"))\n",
    "corp_abb.update(dict.fromkeys([\"REDVLPMNT\", \"REDEVLPMNT\", \"REDEVELPMENT\", \"REDEVELPMNT\", \"REDEVEL\", \"REDEV\"], \"REDEVELOPMENT\"))\n",
    "corp_abb.update(dict.fromkeys([\"AUTH\", \"AUTHOR\", \"AUT\"], \"AUTHORITY\"))\n",
    "corp_abb.update(dict.fromkeys([\"DEV\", \"DVLPMNT\", \"DEVLPMNT\", \"DEVELO\", \"DEVELOPME\", \"DEVLOP\",\"DELVELOPMENT\", \"DEVELOP\", \"DEVELOPMNT\", \"DEVELOPMEN\", \"DEVE\", \"DEVELOPMENTS\"], \"DEVELOPMENT\"))\n",
    "corp_abb.update(dict.fromkeys([\"NAT\", \"NTL\", \"NATL\"], \"NATIONAL\"))\n",
    "corp_abb.update(dict.fromkeys([\"HOLDING\", \"HLDNG\"], \"HOLDINGS\"))\n",
    "corp_abb.update(dict.fromkeys([\"ARCH\", \"ARCHDIOCES\", \"DIOCESE\"], \"ARCHDIOCESE\"))\n",
    "corp_abb.update(dict.fromkeys([\"MNGMT\", \"MGMT\", \"MANAG\", \"MGT\", \"MNGT\", \"MGMNT\", \"MGNT\", \"MANAGE\", \"MNGMNT\", \"MANAGEMNT\", \"MANAGEMEN\", \"MANGEMENT\"], \"MANAGEMENT\"))\n",
    "corp_abb.update(dict.fromkeys([\"MNGRS\", \"MGRS\", \"MANAGRS\", \"MNGR\", \"MGR\", \"MNGMT\", \"MANAGR\", \"MANAGER\"], \"MANAGERS\"))\n",
    "corp_abb.update(dict.fromkeys([\"MORTG\", \"MTG\", \"MTGS\", \"MORTGAGES\", \"MORTGAG\"], \"MORTGAGE\"))\n",
    "corp_abb.update(dict.fromkeys([\"ORGANIZATI\", \"ORG\"], \"ORGANIZATION\"))\n",
    "corp_abb.update(dict.fromkeys([\"ROMAN CATH\"], \"ROMAN CATHOLIC\"))\n",
    "corp_abb.update(dict.fromkeys([\"PTY\", \"PTYS\", \"PTIES\", \"PROP\", \"PROPERTY\", \"PROPERT\", \"PROPERTI\", \"PRPRTY\"], \"PROPERTIES\"))\n",
    "corp_abb.update(dict.fromkeys([\"THE\"], \"\"))\n",
    "corp_abb.update(dict.fromkeys([\"AV\"], \"AVE\"))\n",
    "corp_abb.update(dict.fromkeys([\"REAL EST\"], \"REAL ESTATE\"))\n",
    "corp_abb.update(dict.fromkeys([\"BROS\"], \"BROTHERS\"))\n",
    "corp_abb.update(dict.fromkeys([\"FIRST\", \"FRST\"], \"1ST\"))\n",
    "corp_abb.update(dict.fromkeys([\"SECOND\", \"SECND\", \"SCND\"], \"2ND\"))\n",
    "corp_abb.update(dict.fromkeys([\"THIRD\", \"THRD\"], \"3RD\"))\n",
    "corp_abb.update(dict.fromkeys([\"FIFTH\"], \"5TH\"))\n",
    "corp_abb.update(dict.fromkeys([\"TWENTIETH\"], \"20TH\"))\n",
    "corp_abb.update(dict.fromkeys([\"THIRTIETH\"], \"30TH\"))\n",
    "corp_abb.update(dict.fromkeys([\"FORTIETH\"], \"40TH\"))\n",
    "corp_abb.update(dict.fromkeys([\"FIFTIETH\"], \"50TH\"))\n",
    "corp_abb.update(dict.fromkeys([\"SIXTIETH\"], \"60TH\"))\n",
    "corp_abb.update(dict.fromkeys([\"SEVENTIETH\"], \"70TH\"))\n",
    "corp_abb.update(dict.fromkeys([\"EIGHTIETH\"], \"80TH\"))\n",
    "corp_abb.update(dict.fromkeys([\"NINETIETH\"], \"90TH\"))\n",
    "corp_abb.update(dict.fromkeys([\"HUNDREDTH\"], \"100TH\"))\n",
    "corp_abb.update(dict.fromkeys([\"BLVD\", \"BLVRD\", \"BL\"], \"BOULEVARD\"))\n",
    "corp_abb.update(dict.fromkeys([\"PKWY\", \"PRKWY\", \"PRKWAY\"], \"PARKWAY\"))\n",
    "corp_abb.update(dict.fromkeys([\"LANE\"], \"LN\"))\n",
    "corp_abb.update(dict.fromkeys([\"WRF\",\"WHF\"], \"WARF\"))\n",
    "corp_abb.update(dict.fromkeys([\"TER\", \"TRCE\", \"TERR\"], \"TERRACE\"))\n",
    "corp_abb.update(dict.fromkeys([\"ROAD\", \"RDWY\", \"ROADWAY\"], \"RD\"))\n",
    "corp_abb.update(dict.fromkeys([\"PLACE\", \"PLCE\", \"PLC\"], \"PLACE\"))\n",
    "corp_abb.update(dict.fromkeys([\"CIRCLE\", \"CIRCUIT\", \"CRCT\", \"CC\"], \"CIR\"))\n",
    "corp_abb.update(dict.fromkeys([\"CRT\"], \"COURT\"))\n",
    "corp_abb.update(dict.fromkeys([\"HGWY\", \"HWY\"], \"HIGHWAY\"))\n",
    "corp_abb.update(dict.fromkeys([\"PLZ\", \"PZ\"], \"PLAZA\"))\n",
    "corp_abb.update(dict.fromkeys([\"LANE\", \"LA\"], \"LN\"))\n",
    "corp_abb.update(dict.fromkeys([\"WY\"], \"WAY\"))\n",
    "corp_abb.update(dict.fromkeys([\"CRSNT\", \"CRES\", \"CRESCENT\"], \"CR\"))\n",
    "corp_abb.update(dict.fromkeys([\"ALY\"], \"ALLEY\"))\n",
    "corp_abb.update(dict.fromkeys([\"CWY\"], \"CROSSWAY\"))\n",
    "corp_abb.update(dict.fromkeys([\"DRWY\"], \"DRIVEWAY\"))\n",
    "corp_abb.update(dict.fromkeys([\"SQ\"], \"SQUARE\"))\n",
    "corp_abb.update(dict.fromkeys([\"EXT\"], \"EXTENSION\"))\n",
    "corp_abb.update(dict.fromkeys([\"GARDENS\", \"GRDN\", \"GDNS\"], \"GARDEN\"))\n",
    "corp_abb.update(dict.fromkeys([\"GRN\"], \"GARDEN\"))\n",
    "corp_abb.update(dict.fromkeys([\"HIL\", \"HL\"], \"HILL\"))\n",
    "corp_abb.update(dict.fromkeys([\"ALSTON\"],\"ALLSTON\"))\n",
    "corp_abb.update(dict.fromkeys([\"BACKBAY\"],\"BACK BAY\"))\n",
    "corp_abb.update(dict.fromkeys([\"BCON\", \"BECON\"],\"BEACON\"))\n",
    "corp_abb.update(dict.fromkeys([\"BORSOTN\", \"BOTON\", \"BSTN\", \"BOSTN\"],\"BOSTON\"))\n",
    "corp_abb.update(dict.fromkeys([\"BRGHTON\",\"BRIGHTJTON\",\"BRIGHJTON\",\"BRIGTON\",\"BRIGRTON\",\"BTIGHTON\"],\"BRIGHTON\"))\n",
    "corp_abb.update(dict.fromkeys([\"CHRLESTOWN\",\"CHARLESTOEN\",\"CHARLESTONW\",\"CHARLESTOWEN\",\"CHARLESTWON\", \"CHARLSETOWN\"],\"CHARLESTOWN\"))\n",
    "corp_abb.update(dict.fromkeys([\"CHESNUT\"],\"CHESTNUT\"))\n",
    "corp_abb.update(dict.fromkeys([\"DORCHESER\",\"DORCHESETER\",\"DORCHESTERT\", \"DOCHESTER\",\n",
    "                         \"DORCEHSTER\",\"DORCESTER\",\"DORCH\",\"DORCHERSTER\",\"DORCHESTE\",\"DORCHSTERT\",\n",
    "                         \"DORCHESTON\",\"DORCHESWTER\",\"DORCHETSER\",\"DORCHSETER\",\"DORCHSTER\",\n",
    "                         \"DORHESTER\",\"DORSHESTER\"],\"DORCHESTER\"))\n",
    "corp_abb.update(dict.fromkeys([\"FENMWY\"],\"FENWAY\"))\n",
    "corp_abb.update(dict.fromkeys([\"MATAPABN\",\"MATAPAN\",\"MATTAAPAN\",\"MATTAPN\",\"MATTTAPAN\"],\"MATTAPAN\"))\n",
    "corp_abb.update(dict.fromkeys([\"NORTHEND\"],\"NORTH END\"))\n",
    "corp_abb.update(dict.fromkeys([\"ROLINDALE\",\"ROSINDALE\",\"ROSLINDANLE\",\"ROSLINADLE\",\"ROSLINDAEL\",\"ROSLNDALE\",\n",
    "                         \"ROSLIDANLE\"],\"ROSLINDALE\"))\n",
    "corp_abb.update(dict.fromkeys([\"ROX\", \"TOXBURY\", \"RXBRY\"],\"ROXBURY\"))\n",
    "corp_abb.update(dict.fromkeys([\"SOUTHEND\"],\"SOUTH END\"))\n",
    "corp_abb.update(dict.fromkeys([\"NOR\", \"NRTH\"],\"NORTH\"))\n",
    "corp_abb.update(dict.fromkeys([\"STH\"],\"SOUTH\"))\n",
    "corp_abb.update(dict.fromkeys([\"WST\"],\"WEST\"))\n",
    "corp_abb.update(dict.fromkeys([\"ESTABLISHMENT\"],\"ESTABLISHED\")) \n",
    "corp_abb.update(dict.fromkeys([\"EST\"],\"ESTATE\"))\n",
    "corp_abb.update(dict.fromkeys([\"MA\", \"MAS\", \"MASS\"],\"MASSACHUSETTS\"))\n",
    "corp_abb.update(dict.fromkeys([\"PRES\"],\"PRESIDENT\"))\n",
    "corp_abb.update(dict.fromkeys([\"COM\", \"CMMTY\", \"CMUNITY\", \"COMUNITY\", \"COMMUNITES\", \"COMMUNITIES\"],\"COMMUNITY\"))\n",
    "corp_abb.update(dict.fromkeys([\"EXCHNG\", \"XCHNGE\", \"XCHNG\", \"EXCH\", \"EXC\"],\"EXCHANGE\"))\n",
    "corp_abb.update(dict.fromkeys([\"RENT\", \"RENTALS\", \"RNTL\"],\"RENTAL\"))\n",
    "corp_abb.update(dict.fromkeys([\"RESIDENT\", \"RESIDENTS\", \"RESID\"],\"RESIDENTIAL\"))\n",
    "corp_abb.update(dict.fromkeys([\"COMMWLTH\", \"COMMNWLTH\", \"COMMONWLTH\", \"CMMNWLTH\", \"COMM\"],\"COMMONWEALTH\"))\n",
    "corp_abb.update(dict.fromkeys([\"BY\", \"MDC\", \"DPW\"],\"\"))\n",
    "corp_abb.update(dict.fromkeys([\"REALTY T\"], \"REALTY TRUST\"))\n",
    "corp_abb.update(dict.fromkeys([\"LIMITED PARTNERSHIP\", \"LIMITED PARTNER \", \"LIMITED PARTNERS \"], \"LIMITED\"))\n",
    "corp_abb.update(dict.fromkeys([\"CONDO T\"], \"CONDO TRUST\"))\n",
    "corp_abb.update(dict.fromkeys([\"CO OPERATIVE\", \"COMPANY OP \", \"COOPERATIVE HOUSING CORPORATION \"], \"COOPERATIVE\"))\n",
    "corp_abb.update(dict.fromkeys([\"BOSTON HOUSING AUTH\", \" BHA \"], \"BOSTON HOUSING AUTHORITY\"))\n",
    "corp_abb.update(dict.fromkeys([\"ROMAN CATH\"], \"ROMAN CATHOLIC\"))\n",
    "corp_abb.update(dict.fromkeys([\"MASSACHUSETTS CORPORATION\"], \"\"))\n",
    "corp_abb.update(dict.fromkeys([\"ACQUISITIONS\"], \"ACQUISITION\"))\n",
    "corp_abb.update(dict.fromkeys([\"VENTURES\"], \"VENTURE\"))\n",
    "corp_abb.update(dict.fromkeys([\"DR\"], \"DRIVE\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Common bank errors\n",
    "banks = dict()\n",
    "banks.update(dict.fromkeys(['BANK OF AMERICA', 'BANK OF AMERICA NA','BANK OF AMERICA NATIONAL ASSOCIATION'], \"BANK OF AMERICA\"))\n",
    "banks.update(dict.fromkeys(['BANK OF NEW YORK','BANK OF NEW YORK AS TRUSTEE', 'BANK OF NEW YORK MELLON','BANK OF NEW YORK MELLON CORPORATION''BANK OF NEW YORK TRUST','BANK OF NEW YORK TRUST COMPANY','BANK OF NEW YORK TRUST COMPANY NA', 'BANK OF NEW YORK TRUSTEES'], 'BANK OF NEW YORK'))\n",
    "banks.update(dict.fromkeys(['BANK UNITED', 'BANKUNITED'], \"BANK UNITED\"))\n",
    "banks.update(dict.fromkeys(['CITIBANK', 'CITIBANK NA','CITIBANK NA TRUST', 'CITIBANK NATIONAL ASSOCIATION',], 'CITYBANK'))\n",
    "banks.update(dict.fromkeys(['COUNTRYWIDE BANK','COUNTRYWIDE BANK FSB'], 'COUNTRYWIDE BANK'))\n",
    "banks.update(dict.fromkeys(['DEUTSCHE BNK NATIONAL TRUST COMPANY','DEUSTCHE BANK NATIONAL TRUST COMPANY','DEUTCH BANK NATIONAL TRUST COMPANY','DEUTCHE BANK NATIONAL TRUST COMPANY','DEUTCHE BANK NATIONAL TRUST COMPANY TRUST','DEUTCHE BANK TRUST COMPANY AMERICAS','DEUTSCH BANK NATIONAL TRUST COMPANY', 'DEUTSCHE BANK NATIONAL','DEUTSCHE BANK NATIONAL ASSOCIATION','DEUTSCHE BANK NATIONAL TRUST','DEUTSCHE BANK NATIONAL TRUST COMPANY','DEUTSCHE BANK NATIONAL TRUST TRUST','DEUTSCHE BANK NATN L TRUST COMPANY','DEUTSCHE BANK NATNL TRUST COMPANY','DEUTSCHE BANK TRUST','DEUTSCHE BANK TRUST COMPANY','DEUTSCHE BANK TRUST COMPANY AMERICAS','DEUTSCHE BANK TRUST COMPANY TRUST','DEUTSCHE BANK TRUST NATIONAL','DEUTSCHE NATIONAL BANK TRUST COMPANY TRUST' ], 'DEUSTCHE BANK'))\n",
    "banks.update(dict.fromkeys(['FLAGSTAR BANK','FLAGSTAR BANK FSB' ], 'FLAGSTAR BANK'))\n",
    "banks.update(dict.fromkeys(['HSBC MORTGAGE CORPORATION', 'HSBC MORTGAGE SERVICES INC','HSBC BANK NATIONAL ASSOCIATION', 'HSBC BANK USA','HSBC BANK USA NA', 'HSBC BANK USA NA AS TRUSTEE','HSBC BANK USA NATIONAL', 'HSBC BANK USA NATIONAL ASSOCIATION','HSBC BANK USA NATIONAL ASSOCIATION INC','HSBC BANK USA NATIONAL ASSOCIATION TRUST', 'HSBC BANK USA TRUST',], 'HSBC BANK'))\n",
    "banks.update(dict.fromkeys(['INDYMAC BANK FSB','INDYMAC FEDERAL BANK FSB'], 'INDYMAC BANK'))\n",
    "banks.update(dict.fromkeys(['JP MORGAN CHASE BANK', 'JP MORGAN CHASE BANK TRUST','JPMORGAN CHASE BANK', 'JPMORGAN CHASE BANK NA'], 'JP MORGAN CHASE BANK'))\n",
    "banks.update(dict.fromkeys(['LASALLE BANK NA', 'LASALLE BANK NATIONAL','LASALLE BANK NATIONAL ASSOCIATION','LASALLE BANK NATIONAL ASSOCIATION TRUST'], 'LASALLE BANK'))\n",
    "banks.update(dict.fromkeys(['M AND T BANK','MANDT BANK', 'MANDT BANK SBM'], 'M AND T BANK'))\n",
    "banks.update(dict.fromkeys(['PNC BANK NA', 'PNC BANK NATIONAL ASSOCIATION'], 'PNC BANK'))\n",
    "banks.update(dict.fromkeys(['SANTANDER BANK', 'SANTANDER BANK NA'], 'SANTANDER BANK'))\n",
    "banks.update(dict.fromkeys(['SHAWMUT BANK NATIONAL ASSOCIATION','SHAWMUT BANK OF BOS NA TRUST'], 'SHAWMUT BANK'))\n",
    "banks.update(dict.fromkeys(['SOVEREIGN BANK','SOVEREIGN BANK NA'], 'SOVEREIGN BANK'))\n",
    "banks.update(dict.fromkeys(['TD BANK N NA', 'TD BANK NA'], 'TD BANK'))\n",
    "banks.update(dict.fromkeys(['US BANK AND TRUST','US BANK ASSOCIATION','US BANK NA','US BANK NA TRUSTEE','US BANK NATIONAL','US BANK NATIONAL ASSCO','US BANK NATIONAL ASSOCIATION','US BANK NATIONAL ASSOCIATION T','US BANK NATIONAL ASSOCIATION TRUST','US BANK TRUST','US BANK TRUST NA'], 'US BANK'))\n",
    "banks.update(dict.fromkeys(['WACHOVIA BANK NA','WACHOVIA BANK NA TRUST'], 'WACHOVIA BANK'))\n",
    "banks.update(dict.fromkeys(['WELLS FARGO BANK', 'WELLS FARGO BANK NA','WELLS FARGO BANK NA F/B/O', 'WELLS FARGO BANK NA TRUST','WELLS FARGO BANK NATIONAL','WELLS FARGO BANK NATIONAL ASSOCIATION','WELLS FARGO BANK TRUSTEES'], 'WELLS FARGO BANK'))\n",
    "banks.update(dict.fromkeys(['FEDERAL MATIONAL MORTGAGE','FEDERAL MORTGAGE ASSOCIATION','FEDERAL NATIONAL MORTGAGE','FEDERAL NATIONAL MORTGAGE ASSOCIATION','FEEDERAL HOME LOAN MORTGAGE', 'FANNIE MAE FNMA'], 'FANNIE MAE'))\n",
    "banks.update(dict.fromkeys(['FEDERAL HOME LOAN MORTGAGE','FEEDERAL HOME LOAN MORTGAGE', 'FEDERAL HOME MORTGAGE''FEDERAL HOME LOAN', 'FEDERAL HOME LOAN MANAGEMENT CORPORATION','FEDERAL HOME LOAN MG CORPORATION','FEDERAL HOME LOAN MORT CORPORATION', 'FEDERAL HOME LOAN MORTGAGE','FEDERAL HOME LOAN MORTGAGE ASSOCIATION','FEDERAL HOME LOAN MORTGAGE COMPANY','FEDERAL HOME LOAN MORTGAGE CORPORATION','FEDERAL HOME LOAN MTGE CORPORATION','FEEDERAL HOME LOAN MORTGAGE'], 'FREDDIE MAC'))\n",
    "\n",
    "\n",
    "def fix_banks(text, banks):\n",
    "    try:\n",
    "        return banks[text]\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "def convert_abbreviations_spaces(text, corp_abb_2):\n",
    "    \n",
    "    try:\n",
    "        text = text + ' '\n",
    "        for key in corp_abb_2.keys():\n",
    "            text = re.sub(r'{}'.format(key), str(corp_abb_2[key]), text)\n",
    "            \n",
    "        if text.strip()[-5:] == \"NORTH\":\n",
    "            text = text.strip()[:-5] + ' N'\n",
    "        if text.strip()[-5:] == \"SOUTH\":\n",
    "            text = text.strip()[:-5] + ' S'\n",
    "        if text.strip()[-4:] == \"EAST\":\n",
    "            text = text.strip()[:-4] + ' E'\n",
    "        if text.strip()[-4:] == \"WEST\":\n",
    "            text = text.strip()[:-4] + ' W'\n",
    "        \n",
    "        return text.strip()\n",
    "    except:\n",
    "        return text\n",
    "    \n",
    "corp_abb_2 = dict()\n",
    "corp_abb_2.update(dict.fromkeys([\"REALTY T \"], \"REALTY TRUST\"))\n",
    "corp_abb_2.update(dict.fromkeys([\"LIMITED PARTNERSHIP \", \"LIMITED PARTNER \", \"LIMITED PARTNERS \"], \"LIMITED\"))\n",
    "corp_abb_2.update(dict.fromkeys([\"CONDO T \"], \"CONDO TRUST\"))\n",
    "corp_abb_2.update(dict.fromkeys([\"CO OPERATIVE \", \"COMPANY OP \", \"COOPERATIVE HOUSING CORPORATION \"], \"COOPERATIVE\"))\n",
    "corp_abb_2.update(dict.fromkeys([\"BOSTON HOUSING AUTH \", \" BHA \"], \"BOSTON HOUSING AUTHORITY\"))\n",
    "corp_abb_2.update(dict.fromkeys([\"ROMAN CATH \"], \"ROMAN CATHOLIC\"))\n",
    "corp_abb_2.update(dict.fromkeys([\"MASSACHUSETTS CORPORATION \"], \"\"))\n",
    "corp_abb_2.update(dict.fromkeys([\" N \"], \"NORTH\"))\n",
    "corp_abb_2.update(dict.fromkeys([\" S \"], \"SOUTH\"))\n",
    "corp_abb_2.update(dict.fromkeys([\" E \"], \"EAST\"))\n",
    "corp_abb_2.update(dict.fromkeys([\" W \"], \"WEST\"))\n",
    "corp_abb_2.update(dict.fromkeys([\" 1 \"], \"I\"))\n",
    "corp_abb_2.update(dict.fromkeys(['DEVELOPMENTILIMITED'], 'DEVELOPMENT LIMITED'))\n",
    "\n",
    "\n",
    "\n",
    "# Remove for a core name\n",
    "unique_keys = ['CIR ', 'APARTMENTS ', 'SERVICES ', 'INVESTMENTS ', 'HOLDINGS ', 'LN ', 'COMPANY ',\n",
    "'AUTHORITY ', 'INC ', 'FORECLOSURE ', 'ESTABLISHED ', 'CONDO TRUST ', 'COOPERATIVE ',\n",
    "'PARTNERS ', 'CR ', 'PARTNERSHIP ', 'GROUP ', 'ASSOCIATION ', 'TRUSTEES ', 'TRUST ', 'PROPERTIES ',\n",
    "'MANAGEMENT ', 'SQUARE ', 'MANAGERS ', 'EXCHANGE ', 'REAL ESTATE ', 'DEVELOPMENT ', 'REDEVELOPMENT ',\n",
    "'MORTGAGE ', 'RESIDENTIAL ', 'REALTY TRUST ', 'CORPORATION ', 'LIMITED ', 'LLC ', 'ORGANIZATION ',\n",
    "'REALTY ', 'PRT ', 'VENTURE ', 'RENTAL ', 'UNION ', 'CONDO ']\n",
    "\n",
    "\n",
    "def core_name(text, unique_keys):\n",
    "    try:\n",
    "        text = text + ' '\n",
    "        for key in unique_keys:\n",
    "            text = re.sub(r'{}'.format(key), '', text)\n",
    "        return text.strip()\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "\n",
    "# Common bank errors\n",
    "banks = dict()\n",
    "banks.update(dict.fromkeys(['BANK OF AMERICA', 'BANK OF AMERICA NA','BANK OF AMERICA NATIONAL ASSOCIATION'], \"BANK OF AMERICA\"))\n",
    "banks.update(dict.fromkeys(['BANK OF NEW YORK','BANK OF NEW YORK AS TRUSTEE', 'BANK OF NEW YORK MELLON','BANK OF NEW YORK MELLON CORPORATION''BANK OF NEW YORK TRUST','BANK OF NEW YORK TRUST COMPANY','BANK OF NEW YORK TRUST COMPANY NA', 'BANK OF NEW YORK TRUSTEES'], 'BANK OF NEW YORK'))\n",
    "banks.update(dict.fromkeys(['BANK UNITED', 'BANKUNITED'], \"BANK UNITED\"))\n",
    "banks.update(dict.fromkeys(['CITIBANK', 'CITIBANK NA','CITIBANK NA TRUST', 'CITIBANK NATIONAL ASSOCIATION',], 'CITYBANK'))\n",
    "banks.update(dict.fromkeys(['COUNTRYWIDE BANK','COUNTRYWIDE BANK FSB'], 'COUNTRYWIDE BANK'))\n",
    "banks.update(dict.fromkeys(['DEUTSCHE BNK NATIONAL TRUST COMPANY','DEUSTCHE BANK NATIONAL TRUST COMPANY','DEUTCH BANK NATIONAL TRUST COMPANY','DEUTCHE BANK NATIONAL TRUST COMPANY','DEUTCHE BANK NATIONAL TRUST COMPANY TRUST','DEUTCHE BANK TRUST COMPANY AMERICAS','DEUTSCH BANK NATIONAL TRUST COMPANY', 'DEUTSCHE BANK NATIONAL','DEUTSCHE BANK NATIONAL ASSOCIATION','DEUTSCHE BANK NATIONAL TRUST','DEUTSCHE BANK NATIONAL TRUST COMPANY','DEUTSCHE BANK NATIONAL TRUST TRUST','DEUTSCHE BANK NATN L TRUST COMPANY','DEUTSCHE BANK NATNL TRUST COMPANY','DEUTSCHE BANK TRUST','DEUTSCHE BANK TRUST COMPANY','DEUTSCHE BANK TRUST COMPANY AMERICAS','DEUTSCHE BANK TRUST COMPANY TRUST','DEUTSCHE BANK TRUST NATIONAL','DEUTSCHE NATIONAL BANK TRUST COMPANY TRUST' ], 'DEUSTCHE BANK'))\n",
    "banks.update(dict.fromkeys(['FLAGSTAR BANK','FLAGSTAR BANK FSB' ], 'FLAGSTAR BANK'))\n",
    "banks.update(dict.fromkeys(['HSBC MORTGAGE CORPORATION', 'HSBC MORTGAGE SERVICES INC','HSBC BANK NATIONAL ASSOCIATION', 'HSBC BANK USA','HSBC BANK USA NA', 'HSBC BANK USA NA AS TRUSTEE','HSBC BANK USA NATIONAL', 'HSBC BANK USA NATIONAL ASSOCIATION','HSBC BANK USA NATIONAL ASSOCIATION INC','HSBC BANK USA NATIONAL ASSOCIATION TRUST', 'HSBC BANK USA TRUST',], 'HSBC BANK'))\n",
    "banks.update(dict.fromkeys(['INDYMAC BANK FSB','INDYMAC FEDERAL BANK FSB'], 'INDYMAC BANK'))\n",
    "banks.update(dict.fromkeys(['JP MORGAN CHASE BANK', 'JP MORGAN CHASE BANK TRUST','JPMORGAN CHASE BANK', 'JPMORGAN CHASE BANK NA'], 'JP MORGAN CHASE BANK'))\n",
    "banks.update(dict.fromkeys(['LASALLE BANK NA', 'LASALLE BANK NATIONAL','LASALLE BANK NATIONAL ASSOCIATION','LASALLE BANK NATIONAL ASSOCIATION TRUST'], 'LASALLE BANK'))\n",
    "banks.update(dict.fromkeys(['M AND T BANK','MANDT BANK', 'MANDT BANK SBM'], 'M AND T BANK'))\n",
    "banks.update(dict.fromkeys(['PNC BANK NA', 'PNC BANK NATIONAL ASSOCIATION'], 'PNC BANK'))\n",
    "banks.update(dict.fromkeys(['SANTANDER BANK', 'SANTANDER BANK NA'], 'SANTANDER BANK'))\n",
    "banks.update(dict.fromkeys(['SHAWMUT BANK NATIONAL ASSOCIATION','SHAWMUT BANK OF BOS NA TRUST'], 'SHAWMUT BANK'))\n",
    "banks.update(dict.fromkeys(['SOVEREIGN BANK','SOVEREIGN BANK NA'], 'SOVEREIGN BANK'))\n",
    "banks.update(dict.fromkeys(['TD BANK N NA', 'TD BANK NA'], 'TD BANK'))\n",
    "banks.update(dict.fromkeys(['US BANK AND TRUST','US BANK ASSOCIATION','US BANK NA','US BANK NA TRUSTEE','US BANK NATIONAL','US BANK NATIONAL ASSCO','US BANK NATIONAL ASSOCIATION','US BANK NATIONAL ASSOCIATION T','US BANK NATIONAL ASSOCIATION TRUST','US BANK TRUST','US BANK TRUST NA'], 'US BANK'))\n",
    "banks.update(dict.fromkeys(['WACHOVIA BANK NA','WACHOVIA BANK NA TRUST'], 'WACHOVIA BANK'))\n",
    "banks.update(dict.fromkeys(['WELLS FARGO BANK', 'WELLS FARGO BANK NA','WELLS FARGO BANK NA F/B/O', 'WELLS FARGO BANK NA TRUST','WELLS FARGO BANK NATIONAL','WELLS FARGO BANK NATIONAL ASSOCIATION','WELLS FARGO BANK TRUSTEES'], 'WELLS FARGO BANK'))\n",
    "banks.update(dict.fromkeys(['FEDERAL MATIONAL MORTGAGE','FEDERAL MORTGAGE ASSOCIATION','FEDERAL NATIONAL MORTGAGE','FEDERAL NATIONAL MORTGAGE ASSOCIATION','FEEDERAL HOME LOAN MORTGAGE', 'FANNIE MAE FNMA'], 'FANNIE MAE'))\n",
    "banks.update(dict.fromkeys(['FEDERAL HOME LOAN MORTGAGE','FEEDERAL HOME LOAN MORTGAGE', 'FEDERAL HOME MORTGAGE''FEDERAL HOME LOAN', 'FEDERAL HOME LOAN MANAGEMENT CORPORATION','FEDERAL HOME LOAN MG CORPORATION','FEDERAL HOME LOAN MORT CORPORATION', 'FEDERAL HOME LOAN MORTGAGE','FEDERAL HOME LOAN MORTGAGE ASSOCIATION','FEDERAL HOME LOAN MORTGAGE COMPANY','FEDERAL HOME LOAN MORTGAGE CORPORATION','FEDERAL HOME LOAN MTGE CORPORATION','FEEDERAL HOME LOAN MORTGAGE'], 'FREDDIE MAC'))\n",
    "\n",
    "\n",
    "def fix_banks(text, banks):\n",
    "    try:\n",
    "        return banks[text]\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "\n",
    "    \n",
    "def switch_the(text):\n",
    "    if text[-4:] == ' THE':\n",
    "        return 'THE ' + text[:-4]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_nesw(text):\n",
    "    try:\n",
    "        directions = {'NORTH':' N ', 'SOUTH': ' S ', 'EAST':' E ', 'WEST':' W '}\n",
    "        if any([x for x in ['NORTH', 'SOUTH', 'EAST', 'WEST'] if x in text]):\n",
    "            for direction in [x for x in ['NORTH', 'SOUTH', 'EAST', 'WEST'] if x in text]:\n",
    "                text = text.replace(direction, directions[direction])\n",
    "                text = ' '.join([x.strip() for x in text.split()])\n",
    "        return text\n",
    "    except:\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_address(text):\n",
    "    \n",
    "    try:\n",
    "        # Punc/Spaces/Symbols\n",
    "        text = text.strip()\n",
    "        text = convert_st(text)\n",
    "        text = delete_symbols_spaces(text)\n",
    "        \n",
    "        # Numbers\n",
    "        text_list = combine_numbers([str(words_to_num(x)) for x in text.split()])\n",
    "        text_list = [convert_mixed(x) for x in text_list]\n",
    "        text_list = [words_to_num(x) for x in ' '.join(text_list).split()]\n",
    "        text_list = combine_numbers([words_to_num(x) for x in text_list])\n",
    "        text_list = [convert_ordinals(x) for x in text_list]\n",
    "\n",
    "        # Abbreviations\n",
    "        text_list = [convert_abbreviations(x, corp_abb) for x in text_list]   \n",
    "        text = ' '.join([str(x) for x in text_list]).upper()\n",
    "        text = text.replace(' / ', '/')\n",
    "        \n",
    "        # After abbreviations\n",
    "        text =  re.sub(r'(?<=\\b[A-Z]) (?=[A-Z]\\b)', '', text)\n",
    "\n",
    "        # Misc\n",
    "        # Switch The to the front\n",
    "        text = switch_the(text)\n",
    "        text = convert_abbreviations_spaces(text, corp_abb_2)\n",
    "        text = convert_nesw(text)\n",
    "        text = dedup_words(text)\n",
    "        text = take_first(text)\n",
    "        text = text.replace('-','')\n",
    "        text = drop_floors(text)\n",
    "        text = drop_letters(text)\n",
    "        text = drop_floors(text)\n",
    "\n",
    "\n",
    "        return text\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Remove common issues in addresses\n",
    "def convert_st(text):\n",
    "    try:\n",
    "        text = text.split('#')[0]\n",
    "        text = text.split('APT')[0]\n",
    "        text = text.split('UNIT')[0]\n",
    "        text = text.split('FLOOR')[0]\n",
    "        text = text.split('SUITE')[0]\n",
    "\n",
    "        return text\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "\n",
    "def change_NESW(text):\n",
    "    try:\n",
    "        directions = {'N ': 'NORTH ', 'E ':'EAST ', 'W ':'WEST ', 'S ':'SOUTH '}\n",
    "        if text[:2] in ['E ', 'W ', 'S ','N ']:\n",
    "            text = directions[text[:2]] + text[2:]\n",
    "        return text\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "def convert_zip(text):\n",
    "    try:\n",
    "        text = str(int(text)).zfill(5)\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "    return text\n",
    "\n",
    "def dedup_words(text):\n",
    "    dups = {}\n",
    "    text_list = text.split()\n",
    "    if len(set(text_list)) < len(text_list):\n",
    "        single_words = []\n",
    "        double_words = []\n",
    "        for val in text_list:\n",
    "            if val not in single_words:\n",
    "                single_words.append(val)\n",
    "            else:\n",
    "                double_words.append(val)\n",
    "        de_dups = []\n",
    "\n",
    "        for i, val in enumerate(text_list):\n",
    "            if val in double_words:\n",
    "                de_dups.append(i)\n",
    "\n",
    "        text_list.pop(min(de_dups))\n",
    "    return ' '.join(text_list)\n",
    "\n",
    "\n",
    "\n",
    "def take_first(text):\n",
    "    try:\n",
    "        text = re.findall(r'\\d+-\\d+', text)[0].split('-')[0] + re.sub(r'\\d+-\\d+', '', text)\n",
    "    except:\n",
    "        \n",
    "        return text\n",
    "    return text\n",
    "\n",
    "def drop_letters(text):\n",
    "    try:\n",
    "        text = re.sub(r'\\d+[a-zA-Z]', re.findall(r'\\d+[a-zA-Z]', text)[0][:-1], text)\n",
    "    except:\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "def drop_floors(text):\n",
    "    try:\n",
    "        text = re.sub(r' \\d+D','',text)\n",
    "    except:\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "\n",
    "def process_text_parallel(df):\n",
    "    df['CleanName'] = df.OWNER_NM.apply(lambda x : clean_names(x))\n",
    "    df['CoreName'] = df.CleanName.apply(lambda x : core_name(x, unique_keys))\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_address_parallel(df):\n",
    "\n",
    "    df['MAIL_ADD'] = df['MAIL_ADDRESS'].apply(lambda x: clean_address(x))\n",
    "    df['MAIL_ADD_CS'] = df['MAIL_CITY_STATE'].apply(lambda x: clean_address(x))\n",
    "    df['MAIL_ADD_CS'] = df['MAIL_ADD_CS'].apply(lambda x : change_NESW(x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd08555-a909-43c7-b8a6-a418dfb6c325",
   "metadata": {},
   "source": [
    "Example provided using data from 2015-2016 in Boston, MA. Data provided by the Boston Area Research Initiative: https://dataverse.harvard.edu/dataverse/BARI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3864a9fa-3150-478c-bf82-a9a1af7855ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 190 ms, sys: 60 ms, total: 250 ms\n",
      "Wall time: 8.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Example for cleaning Tax Assessment data\n",
    "resPar = pd.read_csv(\"../data/BostonResidentialParcels2015_2016_Example.csv\")\n",
    "\n",
    "\n",
    "# Takes in OWNER_NM = raw owner name, MAIL_ADD = raw owner address, MAIL_ADD_CS = raw owner city state,\n",
    "# MAIL_ZIPCODE = raw owner zipcode\n",
    "\n",
    "# Returns CleanName = cleaned owner name\n",
    "# CleanAddress = cleaned owner address\n",
    "\n",
    "# CoreName = simplified owner name used for fuzzy-matching later on\n",
    "\n",
    "\n",
    "def process_text_parallel(df):\n",
    "    df['CleanName'] = df.OWNER_NM.apply(lambda x : clean_names(x))\n",
    "    df['CoreName'] = df.CleanName.apply(lambda x : core_name(x, unique_keys))\n",
    "    df['MAIL_ADD'] = df['MAIL_ADDRESS'].apply(lambda x: clean_address(x))\n",
    "    df['MAIL_ADD_CS'] = df['MAIL_CITY_STATE'].apply(lambda x: clean_address(x))\n",
    "    df['MAIL_ADD_CS'] = df['MAIL_ADD_CS'].apply(lambda x : change_NESW(x))\n",
    "    df['MAIL_ZIPCODE'] = df['MAIL_ZIP'].apply(lambda x : convert_zip(x))\n",
    "    df['CleanAddress'] = df['MAIL_ADD'].apply(str) + ' ' + df['MAIL_ADD_CS'].apply(str) \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cleanedData = parallelize_dataframe(resPar, process_text_parallel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b04d7625-3228-4591-b308-7cdb070a8329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzzy-matching names\n",
    "\n",
    "def ngrams(string, n=3):\n",
    "    string = string.encode(\"ascii\", errors=\"ignore\").decode() \n",
    "    string = string.lower()\n",
    "    chars_to_remove = [')', '(', '.', '|', '[', ']', '{', '}', \"'\"]\n",
    "    rx = '[' + re.escape(''.join(chars_to_remove)) + ']'\n",
    "    string = re.sub(rx, '', string) # remove the list of chars defined above\n",
    "    string = string.replace('&', 'and')\n",
    "    string = string.replace(',', ' ').replace('-', ' ')\n",
    "    string = string.title() # Capital at start of each word\n",
    "    string = re.sub(' +',' ',string).strip() # combine whitespace\n",
    "    string = ' ' + string + ' ' # pad\n",
    "    #string = re.sub(r'[,-./]', r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "def combine_columns(c1, c2):\n",
    "    if pd.isnull(c1) == True:\n",
    "        return c2\n",
    "    else:\n",
    "        return c1\n",
    "\n",
    "    \n",
    "\n",
    "def combine_columns_parallel(df):\n",
    "\n",
    "    for column in df.columns:\n",
    "        if column[-2:] == '_x':\n",
    "            df[column[:-2]] = df.apply(lambda x: combine_columns(x[column], x[column[:-2] + '_y']), axis =1)\n",
    "            df.drop(columns = [column, column[:-2] + '_y'], inplace = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Basics - matching based on cosine similiartity on clean name and clean address.\n",
    "# Returns a common name for all the names with greater than 0.85 similarity \n",
    "\n",
    "# Some code from: https://colab.research.google.com/drive/1Z4-cEabpx7HM1pOi49Mdwv7WBOBhn2cl#scrollTo=_LrU12pF4YYJ\n",
    "\n",
    "def fuzzy_match_year(year_df):\n",
    "\n",
    "    # Get cosine similarity and return 3 nearest matches\n",
    "    org_names = list(year_df['NameAddress'].dropna().unique())\n",
    "    vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\n",
    "    tf_idf_matrix = vectorizer.fit_transform(org_names)\n",
    "    messy_names = org_names \n",
    "    messy_tf_idf_matrix = vectorizer.transform(messy_names)\n",
    "    data_matrix = tf_idf_matrix\n",
    "    index = nmslib.init(method='hnsw', space='cosinesimil_sparse_fast', data_type=nmslib.DataType.SPARSE_VECTOR) \n",
    "    index.addDataPointBatch(data_matrix)\n",
    "    index.createIndex() \n",
    "\n",
    "    # Nearest K matches (3 used in this case)\n",
    "    num_threads = 8\n",
    "    K=3\n",
    "    query_matrix = messy_tf_idf_matrix\n",
    "    query_qty = query_matrix.shape[0]\n",
    "    nbrs = index.knnQueryBatch(query_matrix, k = K, num_threads = num_threads)\n",
    "\n",
    "    # Get all matches\n",
    "    mts =[]\n",
    "    for i in range(len(nbrs)):\n",
    "        origional_nm = messy_names[i]\n",
    "        for row in list(range(len(nbrs[i][0]))):\n",
    "            try:\n",
    "                matched_nm   = org_names[nbrs[i][0][row]]\n",
    "                conf         = abs(nbrs[i][1][row])\n",
    "            except:\n",
    "                matched_nm   = \"no match found\"\n",
    "                conf         = None\n",
    "            mts.append([origional_nm,matched_nm,conf])\n",
    "\n",
    "    matches = pd.DataFrame(mts,columns=['OriginalName','MatchedName','conf'])\n",
    "    matches['Ldist'] = matches[['MatchedName', 'OriginalName']].apply(lambda x: lev.distance(x[0], x[1]), axis = 1)\n",
    "    matches['conf1'] = 1- matches['conf']\n",
    "\n",
    "    edges = []\n",
    "    \n",
    "    # Limit to good matches (checked this threshold (.85) based on visual checks - might need to be adapted to corpus)\n",
    "    good_matches = matches[(matches['Ldist'] > 0) & (matches['conf1'] > .85)].sort_values(by  = ['conf1'])\n",
    "    \n",
    "    # Create a graph of all matched pairs to find connected components and then return single name for all \n",
    "    # names in component\n",
    "    for i, row in good_matches.iterrows():\n",
    "        edges.append((row['OriginalName'], row['MatchedName']))\n",
    "\n",
    "    gMatches = nx.Graph()\n",
    "    gMatches.add_edges_from(edges)\n",
    "\n",
    "    combosgMatches = {}\n",
    "    for i, connections in enumerate(list(nx.connected_components(gMatches))):\n",
    "        shortest = min(connections, key=len)\n",
    "        for component in connections:\n",
    "            combosgMatches[component] = shortest\n",
    "\n",
    "    good_matches['NewName'] = good_matches.OriginalName.apply(lambda x: combosgMatches[x])\n",
    "\n",
    "    good_matches = pd.merge(good_matches, year_df[['CleanName', 'CleanAddress']], how = 'left', left_on = 'NewName', right_on  = 'CleanName')\n",
    "    good_matches = pd.merge(good_matches, year_df[['CleanName', 'CleanAddress', 'NameAddress']], how = 'left', left_on = 'NewName', right_on  = 'NameAddress')\n",
    "    good_matches = parallelize_dataframe(good_matches, combine_columns_parallel)\n",
    "    good_matches.rename(columns = {'CleanName':'FuzzyName', 'CleanAddress':'FuzzyAddress'}, inplace = True)\n",
    "\n",
    "    # Keep good matches and join back to data\n",
    "    good_matches.drop_duplicates(subset = ['FuzzyName', 'FuzzyAddress', 'OriginalName'], inplace = True)\n",
    "    \n",
    "    year_df = pd.merge(year_df, good_matches[['FuzzyName', 'FuzzyAddress', 'OriginalName']], how = 'left', left_on = 'NameAddress', right_on  = 'OriginalName')\n",
    "\n",
    "    year_df['FuzzyName'].fillna(year_df['CleanName'], inplace=True)\n",
    "    year_df['FuzzyAddress'].fillna(year_df['CleanAddress'], inplace=True)\n",
    "    \n",
    "    return year_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adc843aa-9184-451e-a3f9-234b0ea66923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 17s, sys: 936 ms, total: 2min 18s\n",
      "Wall time: 18.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Fuzzy-matching example \n",
    "\n",
    "# Note this is split by years to only link WITHIN YEAR\n",
    "\n",
    "cleanedData['NameAddress'] = cleanedData['CleanName'] + ' ' + cleanedData['CleanAddress']\n",
    "\n",
    "for year in [2015, 2016]:\n",
    "    \n",
    "    year_df = cleanedData[cleanedData['YEAR'] == year]\n",
    "\n",
    "    year_df = fuzzy_match_year(year_df)\n",
    "    \n",
    "    if year == 2015:\n",
    "        fuzzyData = year_df\n",
    "    else:\n",
    "        fuzzyData = fuzzyData.append(year_df, ignore_index = True)\n",
    "        \n",
    "        \n",
    "fuzzyData['FuzzyNameAddress'] = fuzzyData['FuzzyName'] + ' ' + fuzzyData['FuzzyAddress']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6db206ef-aa04-4a92-9d11-df2e8a42fc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.12 s, sys: 732 ms, total: 4.85 s\n",
      "Wall time: 6.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Add corporate data in 2 steps\n",
    "# 1) Link owner names to corporate data, exact and fuzzy\n",
    "# 2) Link corporate names to each other by shared individuals, exact and fuzzy\n",
    "\n",
    "# Step 1\n",
    "\n",
    "# Clean corporate data\n",
    "corp_data = pd.read_csv(\"../data/MACorporateDataExample.csv\", low_memory= False)\n",
    "corp_data = corp_data[['DataID', 'FEIN', 'EntityName', 'DateOfOrganization', 'ActiveFlag']]\n",
    "\n",
    "\n",
    "corp_data.replace('', np.nan, inplace=True)\n",
    "corp_data.replace(' ', np.nan, inplace=True)\n",
    "corp_data.replace('0', np.nan, inplace=True)\n",
    "\n",
    "def corp_data_parallel(corp_data):\n",
    "\n",
    "    corp_data['CleanName'] = corp_data.EntityName.apply(lambda x : clean_names(x))\n",
    "    corp_data['CoreName'] = corp_data.CleanName.apply(lambda x : core_name(x, unique_keys))\n",
    "    corp_data['Date'] = pd.to_datetime(corp_data['DateOfOrganization'], errors = 'coerce')\n",
    "    return corp_data\n",
    "\n",
    "\n",
    "corp_data = parallelize_dataframe(corp_data, corp_data_parallel, n_cores = 8)\n",
    "\n",
    "\n",
    "corp_data = corp_data.sort_values(by = ['ActiveFlag', 'Date'],  ascending=[False, False])\n",
    "corp_data = corp_data.drop_duplicates(subset='CleanName', keep='first')\n",
    "\n",
    "\n",
    "fuzzyData['bostonFlag'] = False\n",
    "corp_data['bostonFlag'] = False\n",
    "\n",
    "# Make sure to exclude City of Boston entities from matching\n",
    "# Adapt based on corpus\n",
    "\n",
    "fuzzyData.loc[fuzzyData['CoreName'] == 'CITY OF BOSTON', 'bostonFlag'] = True\n",
    "fuzzyData.loc[fuzzyData['CleanName'] == 'BOSTON HOUSING AUTHORITY', 'bostonFlag'] = True\n",
    "fuzzyData.loc[fuzzyData['CleanName'] == 'BOSTON REDEVELOPMENT AUTHORITY', 'bostonFlag'] = True\n",
    "fuzzyData.loc[fuzzyData['CoreName'] == 'BOSTON RE', 'bostonFlag'] = True\n",
    "fuzzyData.loc[fuzzyData['CoreName'] == 'BOSTON REDEVELOPMNT', 'bostonFlag'] = True\n",
    "fuzzyData.loc[fuzzyData['CoreName'] == 'CITY OF BOSTON MUNICIPAL', 'bostonFlag'] = True\n",
    "\n",
    "fuzzyData.loc[fuzzyData['CleanName'].isin(list(set(banks.values()))), 'bostonFlag'] == True\n",
    "\n",
    "\n",
    "\n",
    "# # matches corp data excluding the boston ones (b/c they don't match on bostonflag)\n",
    "fuzzyData= pd.merge(fuzzyData, corp_data, how= 'left', on = ['CleanName', 'bostonFlag'])\n",
    "\n",
    "\n",
    "fuzzyData = parallelize_dataframe(fuzzyData, combine_columns_parallel)\n",
    "        \n",
    "\n",
    "corp_data.sort_values(by = ['CoreName', 'ActiveFlag', 'Date'], ascending = [False, False, False], inplace = True)\n",
    "cd2 = corp_data.drop_duplicates(subset = ['CoreName'], keep = 'first')\n",
    "\n",
    "# link on core name if there aren't any matches on full name\n",
    "fuzzyData= pd.merge(fuzzyData, cd2, how= 'left', on = ['CoreName', 'bostonFlag'])\n",
    "\n",
    "\n",
    "fuzzyData = parallelize_dataframe(fuzzyData, combine_columns_parallel)\n",
    "\n",
    "\n",
    "# Identify plausible corporations for fuzzy matching\n",
    "\n",
    "corp_words = ['LLC', 'PROPERTIES', 'CHEMICAL', 'PC', 'MD', 'SALON', 'GOOD', 'INSTITUTE', 'HOSPITAL', 'WELLESLEY', 'PROGRAM', 'SHORE', 'TRUSTEES', 'APPLIED',\n",
    "'COASTAL', 'WORLD', 'VENTURES', 'PLYMOUTH', 'HARVARD', 'END', 'BUILDING', 'DELIVERY', 'TOWN', 'YOUTH',\n",
    "'FOODS', 'BLUE', 'GOVERNMENT', 'POST', 'SERVICE', 'EXPORT', 'PACKAGING', 'ISLAND', 'WEALTH', 'ALPHA', '80TH',\n",
    "'CATERING', 'COUNSELING', '50TH', 'ADVISORS', 'ESSEX', 'INDUSTRIAL', 'SOCIAL', 'WESTERN', 'CENTERS', 'FENWAY',\n",
    "'CHESTNUT', 'HOME', 'LINE', 'UNITED', 'SAFETY', 'BACK', 'MATERIALS', 'WEST', 'SUN', 'SCIENCE', 'HOLDINGS', \n",
    "'UNION', '40TH', 'TRANSPORT', 'FLOOR', '90TH', 'BUSINESS', 'ENTERTAINMENT', 'ST', 'FOR', 'STORAGE', \n",
    "'SYSTEMS', 'INVESTORS', 'ENGINEERS', '30TH', 'HOTEL', 'FINE', 'CONSULTANTS', 'PATRIOT', 'SPECIALTY', 'HOSPITALITY',\n",
    "'TRANSPORTATION', 'SUPPLY', 'TRAINING', 'NEW ENGLAND', 'PUBLIC', 'BIG', 'HIGHWAY', 'AGENCY', 'REAL', 'MERRIMACK',\n",
    "'NORTH END', 'HEALTH', 'STATES', 'PARTNERSHIP', 'PLAZA', 'MASSACHUSETTS', 'MUNICIPAL', 'COFFEE', 'COMMONWEALTH',\n",
    "'RESTAURANT', 'WORLDWIDE', 'EYE', 'WINE', 'PLUMBING', 'STRATEGIES', 'PIONEER', 'TIME', 'ALL', 'VINEYARD', 'ROCK',\n",
    "'TRINITY', 'COMPANY', 'CENTRAL', 'COUNCIL', 'CLEAN', 'PARK', 'CABLE', 'EXCHANGE', 'TECHNOLOGIES', 'EDUCATIONAL', \n",
    "'APARTMENTS', 'MATTAPAN', 'SOCCER', 'ELITE', 'GOLF', 'PREMIER', 'ART', 'OCEAN', 'EASTERN', 'STAR', 'FRANKLIN', 'ACADEMY',\n",
    "'GREATER', 'SPECIALISTS', 'CLEANING', 'SMART', 'CAB', 'LOGISTICS', 'CONTRACTORS', 'BROKERAGE', 'LANDSCAPE', \n",
    "'HIGHLAND', 'BEAUTY', 'DIGITAL', 'ADVISORY', 'NETWORKS', 'SILVER', 'MEDICAL', 'CALIFORNIA', 'SALEM', 'TECHNOLOGY', \n",
    "'SALES', 'SYSTEM', 'PASS', 'CONVENIENCE', 'FASHION', 'RESOURCE', 'ESTATE', 'MANUFACTURING', 'CHARITABLE', \n",
    "'PERFORMANCE', 'COMMUNITY', 'MAINTENANCE', 'STUDIO', 'BRIDGE', 'LABS', 'POWER', 'PRODUCTS', 'COUNTRY', 'CAR',\n",
    "'PAINTING', 'AND', 'VALLEY', 'PLLC', 'COLLABORATIVE', 'OAK', 'GOLD', 'BEACON', 'REVOCABLE', 'SOURCE',\n",
    "'BROCKTON', 'PHOTOGRAPHY', 'SOUTH', 'TRI', '3RD', 'WATER', 'LEARNING', 'COM', 'NATIONAL', 'SUPPORT', 'CITY', \n",
    "'DRIVE', 'UNLIMITED', 'LIGHT', 'SOLUTIONS', 'VETERANS', 'DRYWALL', 'POND', 'TAX', 'INVESTMENTS',\n",
    "'RECOVERY', 'ORGANIZATION', 'CONNECTION', 'EXPRESS', 'DISTRIBUTION', 'ARCHDIOCESE', 'GARAGE', 'THERAPEUTICS',\n",
    "'COAST', 'BUILDERS', 'PRECISION', 'RENTALS', 'TELECOMMUNICATIONS', 'KITCHEN', 'RETAIL', 'LABORATORIES',\n",
    "'COMMUNICATIONS', 'MINISTRIES', 'RESIDENTIAL', 'INTERACTIVE', 'AIR', 'NORTHEAST', 'DATA', 'THROUGH', \n",
    "'FOOD', 'BENEFITS', 'NURSING', 'DORCHESTER', 'SCIENTIFIC', 'PLASTERING', 'PET', 'CARPENTRY', 'MDPC',\n",
    "'METAL', 'DRIVEWAY', 'STATE', 'PARTS', 'BOSTON HOUSING AUTHORITY', 'BOULEVARD', 'ASSOCIATES', 'PLEASANT',\n",
    "'SPRINGFIELD', 'TOURS', 'MOTORS', 'PACIFIC', 'FUND', 'AVIATION', 'STRATEGIC', 'PHYSICAL', 'INNOVATIVE', \n",
    "'ENGINEERING', 'NANTUCKET', 'LLC', 'CORNER', 'ATLANTIC', 'CHARLESTOWN', 'CENTER', 'SEAFOOD', 'ELECTRIC', 'GRILL',\n",
    "'ENTERPRISE', 'WALTHAM', 'ENGLAND', 'QUALITY', 'NEWTON', 'CORPORATE', 'PLUS', 'IMPORTS', 'INFORMATION', 'CLASSIC',\n",
    "'EAGLE', 'NET', 'TITLE', 'CREDIT', 'RESOURCES', 'SCHOLARSHIP', 'HILL', 'GRANITE', 'FARMS', 'REAL ESTATE',\n",
    "'FAMILY', 'FITNESS', 'FLOORING', 'COMPANIES', 'EQUIPMENT', 'CONCORD', 'VENTURE', 'GENERAL', 'ROYAL', 'COLLEGE', \n",
    "'DONUTS', 'MEMORIAL', 'SECURITY', 'MIDDLESEX', 'REPAIR', 'GREAT', 'NEWBURY', '70TH', 'NORTH', 'FUNDS', 'INCOME', \n",
    "'THERAPY', 'PRESS', 'NATURAL', 'TERRACE', 'YOUR', 'OIL', 'CHURCH', 'AUTHORITY', 'PRODUCTIONS', 'CROSSWAY',\n",
    "'CONTINENTAL', 'ADVANCED', 'TECHNICAL', 'NEW', 'TRUCK', 'ARCHITECTS', 'CONCEPTS', 'SERIES', 'LEASING', 'CAFE', 'BAY', \n",
    "'HOUSING', '1ST', 'EDGE', 'YORK', 'GLOBAL', 'CONTRACTOR', 'PRINTING', 'FURNITURE', 'HAIR', 'IGLESIA', 'WHOLESALE',\n",
    "'AUTO', 'GARDEN', 'CR', 'CIR', 'SON', 'CARE', 'FRIENDS', 'WORCESTER', 'PROJECT', 'WAY', 'FORECLOSURE', 'BAR',\n",
    "'MOBILE', 'PUBLISHING', 'PRIME', 'FIRE', 'FUNDING', 'PIZZA', 'MANAGEMENT', 'NORTHERN', 'DENTAL', 'NETWORK', \n",
    "'LIBERTY', 'FINANCE', 'STEEL', 'ENVIRONMENTAL', 'REMODELING', 'BOSTON', 'STOP', 'STUDIOS', 'CONDO', 'THEATRE', \n",
    "'MECHANICAL', 'TRADING', 'CONDO TRUST', 'UNIVERSAL', 'HIGH', 'BEST', 'INSURANCE', 'MOTOR', 'GOD', 'METRO', \n",
    "'COLONIAL', 'CONSTRUCTION', 'GAS', 'PHARMACY', 'CHIROPRACTIC', 'VILLAGE', '100TH', 'PRIVATE', 'INC', 'MOUNTAIN', \n",
    "'WOOD', 'MARINE', 'ASSOCIATION', 'SOUTH END', 'EQUITY', 'ACQUISITION', 'CHAPTER', 'PINE', 'IMPROVEMENT', 'BAKERY',\n",
    "'BROADWAY', 'MUSIC', 'LENDING', 'INDEPENDENT', 'PARKWAY', 'CHILDREN', 'CORPORATION', 'LIVING', 'BROTHERS',\n",
    "'SONS', 'SUB', 'REALTY TRUST', 'TRAVEL', 'INTERNATIONAL', 'RECORDS', 'REDEVELOPMENT', 'AVE', 'PLACE', 'CAPE',\n",
    "'DMD', 'WARF', 'ANDOVER', 'INDUSTRIES', 'COMPUTER', 'DIRECT', 'CONTROL', 'COMMERCIAL', 'HALL', 'RESEARCH',\n",
    "'COD', 'SHOP', 'PACKAGE', 'EXECUTIVE', 'PARTNERS', 'COMMITTEE', 'JEWELRY', 'LEAGUE', 'TRADE',\n",
    "'FISHERIES', 'ATHLETIC', 'CLEANERS', 'HOUSE', 'SOLAR', 'HEATING', 'INN', 'ARTS', 'HOCKEY', 'SUMMIT', \n",
    "'DESIGNS', 'TRANS', 'LN', 'SOFTWARE', 'OFFICE', 'HARBOR', 'ENERGY', 'WOODS', 'UNIVERSITY',\n",
    "'WORKS', 'CAMBRIDGE', 'FARM', 'MAIN', '60TH', 'INTERIORS', 'TOP', 'SHOE', 'FISHING', 'PAPER',\n",
    "'FOUNDATION', 'FALL', 'MANAGERS', '20TH', 'TIRE', 'LIFE', 'HOMES', 'IMAGING', 'ROMAN CATHOLIC',\n",
    "'5TH', 'CHOICE', 'TRUCKING', 'ADVERTISING', 'STORES', 'SPORTS', 'STORE', 'DANCE', 'ROSLINDALE',\n",
    "'BACK BAY', 'KIDS', 'RESTORATION', 'DAY', 'GROUP', 'GLASS', 'LIABILITY', 'PROPERTIES', 'BEACH',\n",
    "'WASHINGTON', 'MARKETING', 'LOWELL', 'TOTAL', 'BODY', 'LAND', 'SOCIETY', 'SECURITIES', 'PLANNING', \n",
    "'ROXBURY', 'AMERICAN', 'LIQUORS', 'LANDSCAPING', 'WIRELESS', 'CONSULTING', 'TEAM', 'ICE', 'ACTION',\n",
    "'LLP', 'LIMOUSINE', 'CAPITAL', 'GRAPHICS', 'COVE', 'MASONRY', 'GALLERY', 'CARPET', 'GRACE', 'RD',\n",
    "'ENTERPRISES', 'DESIGN', 'ALLIANCE', 'ADVANTAGE', 'AMERICA', 'SEA', 'EAST', 'PHOENIX', 'DISTRIBUTORS',\n",
    "'MEDIA', 'TOOL', 'TAXI', 'ESTABLISHED', 'DEVELOPMENT', 'BEDFORD', 'CREATIVE', 'LEGAL', 'ELECTRICAL',\n",
    "'EXTENSION', 'COURT', 'DELI', 'BROKERS', 'SQUARE', 'FISH', 'PROFESSIONAL', 'FUEL', 'COLONY', 'PROTECTION',\n",
    "'LIMITED', 'ACCESS', 'CUSTOM', 'FOREST', 'HERITAGE', 'BURLINGTON', 'VIEW', 'INTEGRATED', 'COOPERATIVE',\n",
    "'FRAMINGHAM', 'SPA', 'AUTOMOTIVE', 'SCHOOL', 'OWNER', 'PHARMACEUTICALS', 'OLD', 'COUNTY', 'CONCRETE',\n",
    "'REALTY', 'CLUB', 'WOMEN', 'BERKSHIRE', 'GOLDEN', 'FINANCIAL', 'ALLEY', 'TREE', 'PETROLEUM', 'TECH', \n",
    "'TELECOM', 'RIVER', 'DOG', 'VALUE', 'OFFICES', 'USA', 'PRESIDENT', 'VIDEO', 'RECYCLING', 'RENTAL', \n",
    "'ALLSTON', 'EDUCATION', 'WASTE', 'BRIGHTON', 'SUMMER', 'STAFFING', 'ELECTRONICS', 'ROOFING', 'ASSET', \n",
    "'VISION', 'SERVICES', 'CONTRACTING', 'DMDPC', 'MACHINE', 'FREE', 'WELLNESS', 'PRO', 'ESTATES', 'STATION',\n",
    " 'HEALTHCARE', 'MORTGAGE', 'MARKET', 'TOWING', '2ND', 'TILE', 'PORTFOLIOS', 'ASSETS', 'RESERVES']\n",
    "\n",
    "\n",
    "def identify_corp(text):\n",
    "\n",
    "    try:\n",
    "        all_words = []\n",
    "        for item in text.split():\n",
    "            if item in corp_words:\n",
    "                all_words.append(True)\n",
    "            else:\n",
    "                all_words.append(False)\n",
    "        if True in all_words:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def identify_num(text):\n",
    "    try:\n",
    "        all_words = []\n",
    "        for item in text.split():\n",
    "            try:\n",
    "                t = int(item)\n",
    "                all_words.append(True)\n",
    "            except:\n",
    "                all_words.append(False)\n",
    "        if True in all_words:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def identify_name_pattern(text):\n",
    "    try:\n",
    "        text = text + ' '\n",
    "        if len(re.findall(r'[a-zA-Z]+ [a-zA-Z]+ [a-zA-Z] ', text)) >= 1:\n",
    "            return True\n",
    "        elif len(re.findall(r'[a-zA-Z]+ [a-zA-Z]+ JR|SR ', text)) >= 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def identify_single(text):\n",
    "    try:\n",
    "        te = len(text.split())\n",
    "        if te == 1:\n",
    "            return True\n",
    "        \n",
    "        else: \n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# File of names from Henry Gomory\n",
    "names = pd.read_csv(\"../data/SSA_Names_DB.csv\")\n",
    "names = names[names['Include?'] == 'Yes']\n",
    "names_list = [name.upper() for name in list(names['Name'])]\n",
    "    \n",
    "def identify_person_name(text):\n",
    "    try:\n",
    "        text_list = text.split()\n",
    "        if 1 < len(text_list):\n",
    "            for name in text_list:\n",
    "                if name in names_list:\n",
    "                    return True\n",
    "\n",
    "            return False\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def ident_parallel(all_info):\n",
    "\n",
    "    all_info['Corp_Words'] = all_info.CleanName.apply(lambda x : identify_corp(x))\n",
    "    all_info['Corp_Num'] = all_info.CleanName.apply(lambda x : identify_num(x))\n",
    "    all_info['People_Structure'] = all_info.CleanName.apply(lambda x : identify_name_pattern(x))\n",
    "    all_info['Corp_Single'] = all_info.CleanName.apply(lambda x : identify_single(x))\n",
    "    all_info['People_Names'] = all_info.CleanName.apply(lambda x : identify_person_name(x))\n",
    "\n",
    "\n",
    "    all_info['Corp'] = np.nan\n",
    "    all_info.loc[all_info['People_Structure'] == True, 'Corp'] = False\n",
    "    all_info.loc[all_info['People_Names'] == True, 'Corp'] = False\n",
    "    all_info.loc[all_info['Corp_Words'] == True, 'Corp'] = True\n",
    "    all_info.loc[all_info['Corp_Num'] == True, 'Corp'] = True\n",
    "    all_info.loc[all_info['Corp_Single'] == True, 'Corp'] = True\n",
    "    all_info.loc[(all_info['People_Structure'] == True) & all_info['People_Names'] == True, 'Corp'] = False\n",
    "    return all_info\n",
    "\n",
    "fuzzyData = parallelize_dataframe(fuzzyData, ident_parallel)\n",
    "\n",
    "\n",
    "# Fuzzy matching (see above)\n",
    "no_matches = fuzzyData[(fuzzyData['DataID'].isna() == True) & (fuzzyData['Corp'] != False) & (fuzzyData['bostonFlag'] == False)]\n",
    "corp_data['CleanName'].dropna().unique\n",
    "\n",
    "org_names = list(corp_data['CleanName'].dropna().unique())\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\n",
    "tf_idf_matrix = vectorizer.fit_transform(org_names)\n",
    "messy_names = list(no_matches['FuzzyName'].dropna().unique()) \n",
    "messy_tf_idf_matrix = vectorizer.transform(messy_names)\n",
    "data_matrix = tf_idf_matrix\n",
    "index = nmslib.init(method='hnsw', space='cosinesimil_sparse_fast', data_type=nmslib.DataType.SPARSE_VECTOR) #A bit more conservative (less than 100 different) but way faster\n",
    "index.addDataPointBatch(data_matrix)\n",
    "index.createIndex() \n",
    "\n",
    "# Only match on 1 (not deduplication, but linking, so only find closest)\n",
    "num_threads = 8\n",
    "K=1\n",
    "\n",
    "query_matrix = messy_tf_idf_matrix\n",
    "query_qty = query_matrix.shape[0]\n",
    "nbrs = index.knnQueryBatch(query_matrix, k = K, num_threads = num_threads)\n",
    "\n",
    "\n",
    "mts =[]\n",
    "for i in range(len(nbrs)):\n",
    "    origional_nm = messy_names[i]\n",
    "    for row in list(range(len(nbrs[i][0]))):\n",
    "        try:\n",
    "            matched_nm   = org_names[nbrs[i][0][row]]\n",
    "            conf         = abs(nbrs[i][1][row])\n",
    "        except:\n",
    "            matched_nm   = \"no match found\"\n",
    "            conf         = None\n",
    "        mts.append([origional_nm,matched_nm,conf])\n",
    "\n",
    "matches = pd.DataFrame(mts,columns=['OriginalName','MatchedName','conf'])\n",
    "matches['Ldist'] = matches[['MatchedName', 'OriginalName']].apply(lambda x: lev.distance(x[0], x[1]), axis = 1)\n",
    "matches['conf1'] = 1- matches['conf']\n",
    "\n",
    "good_matches = matches[(matches['Ldist'] > 0) & (matches['conf1'] > .85) & (matches['conf1'] <1)].sort_values(by  = ['conf1'])\n",
    "\n",
    "good_matches= pd.merge(good_matches, corp_data, how= 'left', left_on = \"MatchedName\", right_on = 'CleanName')\n",
    "\n",
    "good_matches.sort_values(by = ['CleanName', 'ActiveFlag', 'Date'], ascending = [False, False, False], inplace = True)\n",
    "good_matches.drop_duplicates(subset = ['OriginalName'], inplace = True)\n",
    "\n",
    "good_matches.rename(columns = {'CleanName': 'FuzzyName_Corp', 'CoreName': 'FuzzyCoreName_Corp'}, inplace = True)\n",
    "\n",
    "\n",
    "fuzzyDataCorp= pd.merge(fuzzyData, good_matches[['DataID', 'FEIN', 'EntityName', 'DateOfOrganization', 'ActiveFlag',\n",
    "       'FuzzyName_Corp', 'FuzzyCoreName_Corp', 'Date', 'bostonFlag', 'OriginalName']], how= 'left', left_on = ['CleanName', 'bostonFlag'], right_on = ['OriginalName', 'bostonFlag'])\n",
    "\n",
    "fuzzyDataCorp = parallelize_dataframe(fuzzyDataCorp, combine_columns_parallel)\n",
    "\n",
    "\n",
    "fuzzyDataCorp['FuzzyName_Corp'].fillna(fuzzyDataCorp['FuzzyName'], inplace=True)\n",
    "\n",
    "fuzzyDataCorp['FuzzyNameAddress_Corp'] = fuzzyDataCorp['FuzzyName_Corp'] + ' ' + fuzzyDataCorp['FuzzyAddress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caa761b9-41cf-41be-bd5d-f776ba1cd49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 s, sys: 269 ms, total: 10.9 s\n",
      "Wall time: 2.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Step 2, linking through individuals\n",
    "\n",
    "# Individual corporate data\n",
    "df_indiv = pd.read_csv('../data/MACorporateIndivExample.csv', low_memory = False)\n",
    "\n",
    "\n",
    "dat_ids = list(fuzzyDataCorp['DataID'].drop_duplicates().dropna())\n",
    "\n",
    "df_indiv = df_indiv[df_indiv.DataID.isin(dat_ids)]\n",
    "\n",
    "\n",
    "df_indiv['FullName'] = df_indiv['FirstName'] + ' ' + df_indiv['LastName']\n",
    "\n",
    "def corp_indiv_parallel(df_indiv):\n",
    "\n",
    "    df_indiv['CleanName'] = df_indiv.FullName.apply(lambda x : clean_names(x))\n",
    "    df_indiv['RES_ADD'] = df_indiv['ResAddr1'].apply(lambda x: clean_address(x))\n",
    "    df_indiv['BUS_ADD'] = df_indiv['BusAddr1'].apply(lambda x: clean_address(x))\n",
    "    \n",
    "    \n",
    "    return df_indiv\n",
    "\n",
    "\n",
    "df_indiv = parallelize_dataframe(df_indiv, corp_indiv_parallel, n_cores = 8)\n",
    "\n",
    "\n",
    "def sum_info(t1,t2,t3,t4):\n",
    "    try:\n",
    "        t1 = str(t1)\n",
    "    except:\n",
    "        t1 = ''\n",
    "    \n",
    "    try:\n",
    "        t2 = str(t2)\n",
    "    except:\n",
    "        t2 = ''        \n",
    "    try:\n",
    "        t3 = str(t3)\n",
    "    except:\n",
    "        t3 = ''\n",
    "    try:\n",
    "        t4 = str(t4)\n",
    "    except:\n",
    "        t4 = ''\n",
    "    return ' '.join([t1,t2,t3,t4])\n",
    "\n",
    "def process_text(text):\n",
    "    text = text.replace('nan', '')\n",
    "    text = text.replace('AS TRUSTEE REVOCABLE TRUST OF 2014', '')\n",
    "    text = text.replace('ESQ', '')\n",
    "    text = text.replace('SAME ABOVE', '')\n",
    "    text = text.replace('SAME', '')\n",
    "    text = text.replace('AS ABOVE', '')\n",
    "    text = text.replace('ABOVE', '')\n",
    "    text = text.replace('SEE DOCUMENT NAMES', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    text = text.replace('U NKNOWN', '')\n",
    "    text = text.replace('UNKNOWN', '')\n",
    "    text = text.replace('RESIGNED', '')\n",
    "    text = text.replace('VACANT', '')\n",
    "    text = text.replace('N ONE', '')\n",
    "    text = text.replace('nan nan', '')\n",
    "    text = text.replace('SAME SAME', '')\n",
    "    text = text.replace('NONE', '')\n",
    "    text = text.replace(',', ' ')\n",
    "    text = text.replace('.', '')\n",
    "    text = text.replace('-', ' ')\n",
    "    \n",
    "    # \n",
    "    text = text.split()\n",
    "    text = [x for x in text if x]    \n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "    \n",
    "# Link based on cleaned name and residential addresss and then \n",
    "# Link based on cleaned name and business address\n",
    "\n",
    "    \n",
    "def process_indiv_parallel(df):\n",
    "        \n",
    "    df['B1'] = ''\n",
    "    df['B2'] = ''\n",
    "    df['INFOres'] = df[['B1', 'CleanName', 'RES_ADD', 'ResCity']].apply(lambda x : sum_info(x[0], x[1], x[2], x[3]), axis = 1)\n",
    "    df['INFObus'] = df[['B1', 'CleanName', 'BUS_ADD', 'BusCity']].apply(lambda x : sum_info(x[0], x[1], x[2], x[3]), axis = 1)\n",
    "    df['INFOresC'] = df['INFOres'].apply(lambda x: process_text(x))\n",
    "    df['INFObusC'] = df['INFObus'].apply(lambda x: process_text(x))\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Matching code\n",
    "df_indiv = parallelize_dataframe(df_indiv, process_indiv_parallel)\n",
    "\n",
    "df_indiv.replace('', np.nan, inplace=True)\n",
    "\n",
    "org_names = list(df_indiv['INFOresC'].dropna().unique())\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\n",
    "tf_idf_matrix = vectorizer.fit_transform(org_names)\n",
    "messy_names = org_names \n",
    "messy_tf_idf_matrix = vectorizer.transform(messy_names)\n",
    "data_matrix = tf_idf_matrix\n",
    "index = nmslib.init(method='hnsw', space='cosinesimil_sparse_fast', data_type=nmslib.DataType.SPARSE_VECTOR) #A bit more conservative (less than 100 different) but way faster\n",
    "index.addDataPointBatch(data_matrix)\n",
    "index.createIndex() \n",
    "\n",
    "num_threads = 8\n",
    "K=3\n",
    "\n",
    "query_matrix = messy_tf_idf_matrix\n",
    "query_qty = query_matrix.shape[0]\n",
    "nbrs = index.knnQueryBatch(query_matrix, k = K, num_threads = num_threads)\n",
    "\n",
    "\n",
    "mts =[]\n",
    "for i in range(len(nbrs)):\n",
    "    origional_nm = messy_names[i]\n",
    "    for row in list(range(len(nbrs[i][0]))):\n",
    "        try:\n",
    "            matched_nm   = org_names[nbrs[i][0][row]]\n",
    "            conf         = abs(nbrs[i][1][row])\n",
    "        except:\n",
    "            matched_nm   = \"no match found\"\n",
    "            conf         = None\n",
    "        mts.append([origional_nm,matched_nm,conf])\n",
    "\n",
    "matches = pd.DataFrame(mts,columns=['OriginalName','MatchedName','conf'])\n",
    "matches['Ldist'] = matches[['MatchedName', 'OriginalName']].apply(lambda x: lev.distance(x[0], x[1]), axis = 1)\n",
    "matches['conf1'] = 1- matches['conf']\n",
    "\n",
    "edges = []\n",
    "\n",
    "good_matches = matches[(matches['Ldist'] > 0) & (matches['conf1'] > .85)].sort_values(by  = ['conf1'])\n",
    "\n",
    "for i, row in good_matches.iterrows():\n",
    "    edges.append((row['OriginalName'], row['MatchedName']))\n",
    "\n",
    "gMatches = nx.Graph()\n",
    "gMatches.add_edges_from(edges)\n",
    "\n",
    "combosgMatches = {}\n",
    "for i, connections in enumerate(list(nx.connected_components(gMatches))):\n",
    "    shortest = min(connections, key=len)\n",
    "    for component in connections:\n",
    "        combosgMatches[component] = shortest\n",
    "\n",
    "good_matches['INFOresFuzzy'] = good_matches.OriginalName.apply(lambda x: combosgMatches[x])\n",
    "good_matches.rename(columns = {'OriginalName': 'INFOresC'}, inplace = True)\n",
    "\n",
    "good_matches.drop_duplicates(subset = ['INFOresC'], inplace = True)\n",
    "df_indiv = pd.merge(df_indiv, good_matches[['INFOresC', 'INFOresFuzzy']], how = 'left', on = 'INFOresC')\n",
    "\n",
    "\n",
    "\n",
    "org_names = list(df_indiv['INFObusC'].dropna().unique())\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\n",
    "tf_idf_matrix = vectorizer.fit_transform(org_names)\n",
    "\n",
    "messy_names = org_names \n",
    "messy_tf_idf_matrix = vectorizer.transform(messy_names)\n",
    "\n",
    "data_matrix = tf_idf_matrix\n",
    "\n",
    "index = nmslib.init(method='hnsw', space='cosinesimil_sparse_fast', data_type=nmslib.DataType.SPARSE_VECTOR)\n",
    "index.addDataPointBatch(data_matrix)\n",
    "index.createIndex() \n",
    "\n",
    "num_threads = 8\n",
    "K=3\n",
    "\n",
    "query_matrix = messy_tf_idf_matrix\n",
    "query_qty = query_matrix.shape[0]\n",
    "nbrs = index.knnQueryBatch(query_matrix, k = K, num_threads = num_threads)\n",
    "\n",
    "\n",
    "mts =[]\n",
    "for i in range(len(nbrs)):\n",
    "    origional_nm = messy_names[i]\n",
    "    for row in list(range(len(nbrs[i][0]))):\n",
    "        try:\n",
    "            matched_nm   = org_names[nbrs[i][0][row]]\n",
    "            conf         = abs(nbrs[i][1][row])\n",
    "        except:\n",
    "            matched_nm   = \"no match found\"\n",
    "            conf         = None\n",
    "        mts.append([origional_nm,matched_nm,conf])\n",
    "\n",
    "matches = pd.DataFrame(mts,columns=['OriginalName','MatchedName','conf'])\n",
    "matches['Ldist'] = matches[['MatchedName', 'OriginalName']].apply(lambda x: lev.distance(x[0], x[1]), axis = 1)\n",
    "matches['conf1'] = 1- matches['conf']\n",
    "\n",
    "edges = []\n",
    "\n",
    "good_matches = matches[(matches['Ldist'] > 0) & (matches['conf1'] > .85)].sort_values(by  = ['conf1'])\n",
    "\n",
    "for i, row in good_matches.iterrows():\n",
    "    edges.append((row['OriginalName'], row['MatchedName']))\n",
    "\n",
    "gMatches = nx.Graph()\n",
    "gMatches.add_edges_from(edges)\n",
    "\n",
    "combosgMatches = {}\n",
    "for i, connections in enumerate(list(nx.connected_components(gMatches))):\n",
    "    shortest = min(connections, key=len)\n",
    "    for component in connections:\n",
    "        combosgMatches[component] = shortest\n",
    "\n",
    "good_matches['INFObusFuzzy'] = good_matches.OriginalName.apply(lambda x: combosgMatches[x])\n",
    "good_matches.rename(columns = {'OriginalName': 'INFObusC'}, inplace = True)\n",
    "\n",
    "good_matches.drop_duplicates(subset = ['INFObusC'], inplace = True)\n",
    "df_indiv = pd.merge(df_indiv, good_matches[['INFObusC', 'INFObusFuzzy']], how = 'left', on = 'INFObusC')\n",
    "\n",
    "df_indiv['INFOresFuzzy'].fillna(df_indiv['INFOresC'], inplace=True)\n",
    "df_indiv['INFObusFuzzy'].fillna(df_indiv['INFObusC'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_grouping(dataID, combos):\n",
    "    try:\n",
    "        return combos[dataID]\n",
    "    except:\n",
    "        return dataID\n",
    "\n",
    "def add_owner_groups(name, val):\n",
    "    if type(val) != str:\n",
    "        if np.isnan(val) == True:\n",
    "            return name\n",
    "        else:\n",
    "            return val\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "def ident_owner_groups(val):\n",
    "    try:\n",
    "        if 'NameResBus' in val:\n",
    "            return val\n",
    "        else:\n",
    "            return np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def yearGraph(all_info, df_indiv):\n",
    "    \n",
    "    dat_ids = list(all_info['DataID'].drop_duplicates().dropna())\n",
    "\n",
    "    df_all = df_indiv[df_indiv.DataID.isin(dat_ids) == True]\n",
    "    df_all.replace('', np.nan, inplace = True)\n",
    "\n",
    "\n",
    "    nameRES = df_all[['INFOresFuzzy', 'DataID']]\n",
    "    nameRES.dropna(subset = ['INFOresFuzzy'], inplace = True)\n",
    "\n",
    "    nameRES = nameRES.groupby('INFOresFuzzy')['DataID'].apply(list).reset_index(name='Corps')\n",
    "    nameRES['len'] = nameRES['Corps'].apply(lambda x: len(set(x)))\n",
    "    nameRES['set'] = nameRES['Corps'].apply(lambda x: set(x))\n",
    "\n",
    "\n",
    "\n",
    "    nameBUS = df_all[['INFObusFuzzy', 'DataID']]\n",
    "    nameBUS.dropna(subset = ['INFObusFuzzy'], inplace = True)\n",
    "\n",
    "    nameBUS = nameBUS.groupby('INFObusFuzzy')['DataID'].apply(list).reset_index(name='Corps')\n",
    "    nameBUS['len'] = nameBUS['Corps'].apply(lambda x: len(set(x)))\n",
    "    nameBUS['set'] = nameBUS['Corps'].apply(lambda x: set(x))\n",
    "\n",
    "\n",
    "\n",
    "    links = list(nameRES['set'])\n",
    "    links.extend(nameBUS['set'])\n",
    "\n",
    "\n",
    "    edges = []\n",
    "\n",
    "    for nodes in links:\n",
    "        t = list(itertools.combinations(nodes, 2))\n",
    "        for x in t:\n",
    "            edges.append(x)\n",
    "\n",
    "\n",
    "    gNameResBus = nx.Graph()\n",
    "    gNameResBus.add_edges_from(edges)\n",
    "    \n",
    "    # Code to remove articulation points\n",
    "    for tg in [gNameResBus.subgraph(c).copy() for c in nx.connected_components(gNameResBus)]:\n",
    "    \n",
    "        artic_points = []\n",
    "\n",
    "        for name in list(nx.articulation_points(tg)):\n",
    "            artic_points.append((name, len(list(tg.edges(name)))))\n",
    "\n",
    "        artic_points.sort(key = lambda x: x[1], reverse = True)\n",
    "\n",
    "        for artic, val  in artic_points:\n",
    "\n",
    "\n",
    "            edges = list(tg.neighbors(artic))\n",
    "\n",
    "            if len(edges) > 3:\n",
    "                tg.remove_node(artic)\n",
    "                cc = [tg.subgraph(c).copy() for c in nx.connected_components(tg)]\n",
    "                max_v = 0\n",
    "                max_i = None\n",
    "                max_e = None\n",
    "\n",
    "                for i, component in enumerate(cc):\n",
    "                    good_e = [x for x in edges if x in component.nodes()]\n",
    "                    lenx = len(good_e)\n",
    "                    if lenx > max_v:\n",
    "                        max_v = lenx\n",
    "                        max_i = i\n",
    "                        max_e = good_e\n",
    "                    elif lenx == max_v:\n",
    "                        if val < 10:\n",
    "                            try:\n",
    "                                max_e.extend(good_e)\n",
    "                            except:\n",
    "                                max_e = good_e\n",
    "                                max_i = i\n",
    "\n",
    "                if max_e is not None:\n",
    "                    gNameResBus.remove_node(artic)\n",
    "\n",
    "                    edge_back = []        \n",
    "                    for node in max_e:\n",
    "                        edge_back.append((node, artic))\n",
    "                    gNameResBus.add_edges_from(edge_back)\n",
    "                \n",
    "\n",
    "\n",
    "    combosgNameResBus = {}\n",
    "    for i, connections in enumerate(list(nx.connected_components(gNameResBus))):\n",
    "\n",
    "        for component in connections:\n",
    "            combosgNameResBus[component] = str(i) + 'gNameResBus' + str(list(set(all_info['YEAR']))[0])\n",
    "\n",
    "    all_info['Owner_Groups'] = all_info['DataID'].apply(lambda x: get_grouping(x, combosgNameResBus))\n",
    "    all_info['Owner_Groups_All'] = all_info[['CleanName', 'Owner_Groups']].apply(lambda x: add_owner_groups(x[0], x[1]), axis = 1)\n",
    "    all_info['Owner_Groups_Fuzzy'] = all_info[['FuzzyName', 'Owner_Groups']].apply(lambda x: add_owner_groups(x[0], x[1]), axis = 1)\n",
    "    all_info['Owner_Groups_FuzzyCorp'] = all_info[['FuzzyName_Corp', 'Owner_Groups']].apply(lambda x: add_owner_groups(x[0], x[1]), axis = 1)\n",
    "    all_info['Owner_Groups_FuzzyCorpAddress'] = all_info[['FuzzyNameAddress_Corp', 'Owner_Groups']].apply(lambda x: add_owner_groups(x[0], x[1]), axis = 1)\n",
    "\n",
    "    all_info['Only_Groups'] = all_info['Owner_Groups'].apply(lambda x: ident_owner_groups(x))\n",
    "    \n",
    "                    \n",
    "                    \n",
    "    \n",
    "    \n",
    "    return all_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5765a330-83e9-438c-85a8-04866cdbedb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Linking example, again split by year\n",
    "\n",
    "def remove_trusts(text):\n",
    "    try:\n",
    "        text = text.replace('TRUST', '')\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "mask = (fuzzyDataCorp['Corp'] != True)\n",
    "z_valid = fuzzyDataCorp[mask]\n",
    "\n",
    "fuzzyDataCorp.loc[mask, 'CleanName'] = z_valid['CleanName'].apply(lambda x: remove_trusts(x))\n",
    "fuzzyDataCorp.loc[mask, 'FuzzyName'] = z_valid['FuzzyName'].apply(lambda x: remove_trusts(x))\n",
    "fuzzyDataCorp.loc[mask, 'FuzzyName_Corp'] = z_valid['FuzzyName_Corp'].apply(lambda x: remove_trusts(x))\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fuzzyDataCorp['YEAR'] = fuzzyDataCorp['YEAR'].astype(np.int64)\n",
    "\n",
    "for year in [2015,2016]:\n",
    "    if year ==2015:\n",
    "        \n",
    "        res_year = fuzzyDataCorp[fuzzyDataCorp['YEAR'] == year]\n",
    "\n",
    "        res_year = yearGraph(res_year, df_indiv) #, yearGraph, n_cores = 1)\n",
    "        \n",
    "        linked_all = res_year\n",
    "            \n",
    "    else:\n",
    "        res_year = fuzzyDataCorp[fuzzyDataCorp['YEAR'] == year]\n",
    "        \n",
    "        res_year = yearGraph(res_year, df_indiv)\n",
    "\n",
    "        linked_all = linked_all.append(res_year, ignore_index = True)\n",
    "        \n",
    "        \n",
    "# linked_all is the final deduped, linked data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51f0bf1-6da4-4ada-ad6d-4cf639104879",
   "metadata": {},
   "source": [
    "*Note: Not all code is original. Attributions are provided where I remember. I've used and modified code from all over the internet. A big thanks to all the folks over at stackoverflow.* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geo_env]",
   "language": "python",
   "name": "conda-env-geo_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
